---
title: "[Dacon 기업 성공 예측] Catboost Hyper Parameters"
categories: Model_Training&Tuning
tags: [python, Dacon, Catboost, Hyper_Parameters]
---

# Dacon 기업 성공 예측 Hyper Parameters 

---

# Hyper Parametesr 이전 

```python
# ✅ Feature, Target 정의
X = train.drop(columns=['성공확률'])
y = train['성공확률']

# ✅ 모델 저장용 리스트
models = []

# ✅ 5-Fold 교차검증
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# ✅ 예측값 저장용
val_scores = []

for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
    print(f"🔁 Fold {fold+1}/5")

    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # ✅ CatBoostRegressor 생성
    model = CatBoostRegressor(
        iterations=1000,
        learning_rate=0.05,
        depth=6,
        loss_function='MAE',
        early_stopping_rounds=50,
        verbose=100,
        random_state=42
    )

    # ✅ 학습
    model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)

    # ✅ 모델 저장
    models.append(model)

    # ✅ 검증 점수 저장 (MAE 기준)
    val_pred = model.predict(X_val)
    score = mean_absolute_error(y_val, val_pred)
    print(f"✅ Fold {fold+1} MAE: {score:.5f}")
    val_scores.append(score)

# ✅ 전체 평균 점수 출력
print(f"\n📊 평균 MAE: {np.mean(val_scores):.5f}")
```

    (중략)
    📊 평균 MAE: 0.20293

## Hyper Parameters 이전 Review

```
✅ Hyper Parameter 하기 전 기록입니다. 
    ◾ iterations = 1000         # 최대 1000번 학습
    ◾ learning_rate = 0.05      # 학습률
    ◾ depth = 6                 # 트리 깊이
    ◾ loss_function = 'MAE'     # Mean Absolute Error 기준으로 학습
    ◾ early_stopping_round = 50 # 50번 동안 개선이 없으면 학습 중단
    ◾ verbose = 100             # 100회마다 로그 출력
    ◾ random_state = 42         # 재현성을 위해 랜덤 시드 고정정

✅ 각 Fold별 MAE 점수를 계산하고 평균을 구했습니다. 
📊 평균 MAE: 0.20293
```

# 1차 Hyper Parameters

```python 
# ✅ Feature, Target 정의
X = train.drop(columns=['성공확률'])
y = train['성공확률']

# ✅ Optuna objective 함수 정의
def objective(trial):
    params = {
        'iterations': 1000,
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'depth': trial.suggest_int('depth', 4, 10),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),
        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),
        'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0, log=True),
        'loss_function': 'MAE',
        'early_stopping_rounds': 50,
        'verbose': 0,
        'random_state': 42
    }

    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    mae_list = []

    for train_idx, valid_idx in kf.split(X):
        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

        train_pool = Pool(X_train, y_train)
        valid_pool = Pool(X_valid, y_valid)

        model = CatBoostRegressor(**params)
        model.fit(train_pool, eval_set=valid_pool, use_best_model=True)

        preds = model.predict(valid_pool)
        mae = mean_absolute_error(y_valid, preds)
        mae_list.append(mae)

    return np.mean(mae_list)

# ✅ 튜닝 시작
sampler = optuna.samplers.TPESampler(seed=42)
study = optuna.create_study(direction='minimize', sampler=sampler)
study.optimize(objective, n_trials=50)

# ✅ 결과 출력
print("Best trial:")
print(study.best_trial.params)
```

    (중략)
    Best trial:
    {'learning_rate': 0.03249364993947322, 'depth': 8, 'l2_leaf_reg': 4.26944855321362, 'bagging_temperature': 0.12969514204225474, 'random_strength': 0.18317580018607582}

```python 
best_params = study.best_trial.params.copy()

# ✅ 
best_params.update({
    'iterations': 1000,
    'loss_function': 'MAE',
    'early_stopping_rounds': 50,
    'verbose': 100,
    'random_state': 42
})

# ✅ 모델 저장용 리스트
models = []

# ✅ 5-Fold 교차검증
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# ✅ 예측값 저장용
val_scores = []

for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
    print(f"🔁 Fold {fold+1}/5")

    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # ✅ Pool 객체 정의
    train_pool = Pool(X_train, y_train)
    val_pool = Pool(X_val, y_val)

    # ✅ 모델 정의
    model = CatBoostRegressor(**best_params)
    model.fit(train_pool, eval_set=val_pool, use_best_model=True)

    # ✅ 모델 저장
    models.append(model)

    # ✅ 검증 점수 저장 (MAE 기준)
    val_pred = model.predict(val_pool)
    score = mean_absolute_error(y_val, val_pred)
    print(f"✅ Fold {fold+1} MAE: {score:.5f}")
    val_scores.append(score)

# ✅ 전체 평균 점수 출력
print(f"\n📊 평균 MAE (튜닝된 CatBoost): {np.mean(val_scores):.5f}")
```

    (중략)
    📊 평균 MAE (튜닝된 CatBoost): 0.20205

## 1차 Hyper Parameters Review

```
✅ 1차 Hyper Parameter 기록입니다. 
    ◾ Optuna로 자동화된 튜닝입니다. 
    ◾ 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3)
        ▪ 학습률을 범위(0.01 ~ 0.3)에서 튜닝합니다. 
        ▪ 너무 크면 -> 빠르게 배우지만 과적합 위험 
        ▪ 너무 작으면 -> 천천히 배우지만 안정적 
        ▪ 결과 : 약 0.0325 (Hyper Parameters 전 : 0.05)
    ◾ 'depth': trial.suggest_int('depth', 4, 10)
        ▪ 각 트리의 최대 깊이를 4에서 10 사이에서 튜닝합니다. 
        ▪ 너무 크면 -> 복잡한 패턴을 잘 학습하지만 과적합 위험 
        ▪ 너무 작으면 -> 단순한 모델, 덜 과적합
        ▪ 결과 : 8로 선택 (Hyper Parameters 전 : 6)
    ◾ l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True)
        ▪ L2 정규화 강도 (과적합 방지용)을 범위(0.001 ~ 10)에서 log scale로 튜닝합니다. 
        ▪ 높이면 -> 모델을 덜 복잡하게 (과적합 방지)
        ▪ 낮추면 -> 더 복잡하게 학습 가능 
        ▪ 결과 : 4.27정도로 선택
    ◾ 'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0)
        ▪ 데이터 샘플링에 randomness를 주는 정도로 범위 (0.0 ~ 1.0)에서 튜닝합니다. 
        ▪ 0에 가까울수록 -> 보수적으로 샘플링 (덜 무작위)
        ▪ 1에 가까울수록 -> 무작위성이 커짐 
        ▪ 결과 : 0.13점도로 선택 
    ◾ 'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0, log=True)
        ▪ 트리 분할 시 랜덤성 추가 강도로 범위 (0.001 ~ 10)에서 log scale로 튜닝합니다.
        ▪ 높으면 -> 분할할 때 randomness를 더 넣음 (다양성 증가)
        ▪ 낮으면 -> 더 정직하게 최적 분할 찾음
        ▪ 결과 : 튜닝 후 0.18정도로 선택 

📊 평균 MAE: 0.20205
✅ 소폭 상승한 모습입니다. 
```

# 2차 Hyper Parameters

```python 
# ✅ Feature, Target 정의
X = train.drop(columns=['성공확률'])
y = train['성공확률']

# ✅ Optuna objective 함수 정의
def objective(trial):
    params = {
        'iterations': 3000,
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'depth': trial.suggest_int('depth', 4, 10),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),
        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),
        'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0, log=True),
        'border_count': trial.suggest_int('border_count', 64, 254),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 20),
        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),
        'loss_function': 'MAE',
        'early_stopping_rounds': 100,
        'verbose': 0,
        'random_state': 42
    }


    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    mae_list = []

    for train_idx, valid_idx in kf.split(X):
        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

        train_pool = Pool(X_train, y_train)
        valid_pool = Pool(X_valid, y_valid)

        model = CatBoostRegressor(**params)
        model.fit(train_pool, eval_set=valid_pool, use_best_model=True)

        preds = model.predict(valid_pool)
        mae = mean_absolute_error(y_valid, preds)
        mae_list.append(mae)

    return np.mean(mae_list)

# ✅ 튜닝 시작
sampler = optuna.samplers.TPESampler(seed=42)
study = optuna.create_study(direction='minimize', sampler=sampler)
study.optimize(objective, n_trials=50)

# ✅ 결과 출력
print("Best trial:")
print(study.best_trial.params)
```

    (중략)
    Best trial:
    {'learning_rate': 0.028334641989823557, 'depth': 10, 'l2_leaf_reg': 0.0010173074173633859, 'bagging_temperature': 0.3787135134452023, 'random_strength': 0.30665794801197754, 'border_count': 218, 'min_data_in_leaf': 12, 'grow_policy': 'Depthwise'}

```python

# ✅ best_params 가져오기
best_params = study.best_trial.params.copy()

# ✅ 추가로 필요한 고정 파라미터 넣기
best_params.update({
    'iterations': 3000,
    'loss_function': 'MAE',
    'early_stopping_rounds': 50,
    'verbose': 100,
    'random_state': 42
})

# ✅ 모델 저장용 리스트
models = []

# ✅ 5-Fold 교차검증
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# ✅ 예측값 저장용
val_scores = []

for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
    print(f"🔁 Fold {fold+1}/5")

    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # ✅ Pool 객체 정의
    train_pool = Pool(X_train, y_train)
    val_pool = Pool(X_val, y_val)

    # ✅ 모델 정의
    model = CatBoostRegressor(**best_params)
    model.fit(train_pool, eval_set=val_pool, use_best_model=True)

    # ✅ 모델 저장
    models.append(model)

    # ✅ 검증 점수 저장 (MAE 기준)
    val_pred = model.predict(val_pool)
    score = mean_absolute_error(y_val, val_pred)
    print(f"✅ Fold {fold+1} MAE: {score:.5f}")
    val_scores.append(score)

# ✅ 전체 평균 점수 출력
print(f"\n📊 평균 MAE (튜닝된 CatBoost): {np.mean(val_scores):.5f}")
```

    (중략)
    📊 평균 MAE (튜닝된 CatBoost): 0.19909

## 2차 Hyper Parameters Review

```
✅ 1차 Hyper Parameters 와 외부 차이점은 Feature Engineering을 추가했습니다. 

✅ 1차 Hyper Parameters 와 차이점 입니다. 
    ◾ iterations 1000 -> 3000
        ▪ 더 많은 트리 생성 기회를 줘서 복잡한 패턴도 캐치할 수 있도록 합니다. 
    ◾ early_stopping_rounds 50 -> 100
        ▪ 학습이 천천히 좋아질 수 있으니 증가시킵니다. 
    ◾ border_count 추가 튜닝
        ▪ feature를 얼마나 fine하게 나눌지 튜닝 (모델 정밀도 향상 가능)
    ◾ min_data_in_leaf 추가 튜닝
        ▪ 과적합(overfitting) 조절 (잎사귀에 최소 데이터 수 조절)
    ◾ grow_policy 추가 튜닝 
        ▪ 트리 자라는 방식 자체를 튜닝해서 최적의 구조 찾기 (Depthwise vs Lossgiuide 등)
```

# 3차 Hyper Parameters 

```python
# ✅ Feature, Target 정의
X = train.drop(columns=['성공확률'])
y = train['성공확률']

# ✅ Optuna objective 함수 정의
def objective(trial):
    params = {
        'iterations': 5000,
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
        'depth': trial.suggest_int('depth', 4, 10),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 10.0, log=True),
        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),
        'random_strength': trial.suggest_float('random_strength', 0.01, 10.0, log=True),
        'border_count': trial.suggest_int('border_count', 32, 255),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 32),
        'grow_policy': 'SymmetricTree',
        'boosting_type': trial.suggest_categorical('boosting_type', ['Plain', 'Ordered']),
        'loss_function': 'MAE',
        'early_stopping_rounds': 200,
        'verbose': 0,
        'random_state': 42
    }



    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    mae_list = []

    for train_idx, valid_idx in kf.split(X):
        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

        train_pool = Pool(X_train, y_train)
        valid_pool = Pool(X_valid, y_valid)

        model = CatBoostRegressor(**params)
        model.fit(train_pool, eval_set=valid_pool, use_best_model=True)

        preds = model.predict(valid_pool)
        mae = mean_absolute_error(y_valid, preds)
        mae_list.append(mae)

    return np.mean(mae_list)

# ✅ 튜닝 시작
sampler = optuna.samplers.TPESampler(seed=42)
study = optuna.create_study(direction='minimize', sampler=sampler)
study.optimize(objective, n_trials=50)

# ✅ 결과 출력
print("Best trial:")
print(study.best_trial.params)
```

    (중략)
    Best trial:
    {'learning_rate': 0.011049202253484457, 'depth': 9, 'l2_leaf_reg': 1.3199942261535018, 'bagging_temperature': 0.7290071680409873, 'random_strength': 2.059733535743719, 'border_count': 48, 'min_data_in_leaf': 12, 'boosting_type': 'Ordered'}

```python

# ✅ best_params 가져오기
best_params = study.best_trial.params.copy()

# ✅ 추가로 필요한 고정 파라미터 넣기
best_params.update({
    'iterations': 5000,
    'loss_function': 'MAE',
    'early_stopping_rounds': 200,
    'verbose': 100,
    'random_state': 42
})

# ✅ 모델 저장용 리스트
models = []

# ✅ 5-Fold 교차검증
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# ✅ 예측값 저장용
val_scores = []

for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
    print(f"🔁 Fold {fold+1}/5")

    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # ✅ Pool 객체 정의
    train_pool = Pool(X_train, y_train)
    val_pool = Pool(X_val, y_val)

    # ✅ 모델 정의
    model = CatBoostRegressor(**best_params)
    model.fit(train_pool, eval_set=val_pool, use_best_model=True)

    # ✅ 모델 저장
    models.append(model)

    # ✅ 검증 점수 저장 (MAE 기준)
    val_pred = model.predict(val_pool)
    score = mean_absolute_error(y_val, val_pred)
    print(f"✅ Fold {fold+1} MAE: {score:.5f}")
    val_scores.append(score)

# ✅ 전체 평균 점수 출력
print(f"\n📊 평균 MAE (튜닝된 CatBoost): {np.mean(val_scores):.5f}")
```

    (중략)
    📊 평균 MAE (튜닝된 CatBoost): 0.20051

## 3차 Hyper Parameters Review

```
✅ 2차 Hyper Parameters 와 차이점 입니다. 
    ◾ iterations 3000 -> 5000 
        ▪ 더 많은 학습을 시도하려고 함 
    ◾ learning_rate 0.3 -> 0.2 
        ▪ 최대값을 줄여 더 세밀한 학습을 유도 
    ◾ l2_leaf_reg 1e-3 -> 0.01 
        ▪ 최소값을 늘려 더 강한 규제를 사용. 과적합을 방지하려는 의도 
    ◾ random_strength 1e-3 -> 0.01
        ▪ 모델에 더 많은 랜덤성을 추가 
    ◾ border_count 64 ~ 254 에서 32 ~ 255 범주 변경
        ▪ 단순화된 binning을 사용 
    ◾ min_data_in_leaf 20 -> 32
        ▪ 최대값을 확대하며 더 넒은 범위에서 조정 가능
    ◾ grow_policy 다양한 옵션 (SymmetricTree, Depthwise, Lossguide)에서 SymmetricTree로 고정
        ▪ 과적합 방지
    ◾ early_stopping_rounds 100 -> 200
        ▪ 조기 종료 기준을 강화 
```

# 4차 Hyper Parameters 

```python 
# ✅ Feature, Target 정의
X = train.drop(columns=['성공확률'])
y = train['성공확률']

# ✅ Optuna objective 함수 정의
def objective(trial):
    params = {
        'iterations': 3000,
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'depth': trial.suggest_int('depth', 4, 10),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),
        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),
        'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0, log=True),
        'border_count': trial.suggest_int('border_count', 64, 254),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 20),
        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),
        'loss_function': 'MAE',
        'early_stopping_rounds': 100,
        'verbose': 0,
        'random_state': 42
    }


    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    mae_list = []

    for train_idx, valid_idx in kf.split(X):
        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

        train_pool = Pool(X_train, y_train)
        valid_pool = Pool(X_valid, y_valid)

        model = CatBoostRegressor(**params)
        model.fit(train_pool, eval_set=valid_pool, use_best_model=True)

        preds = model.predict(valid_pool)
        mae = mean_absolute_error(y_valid, preds)
        mae_list.append(mae)

    return np.mean(mae_list)

# ✅ 튜닝 시작
sampler = optuna.samplers.TPESampler(seed=42)
study = optuna.create_study(direction='minimize', sampler=sampler)
study.optimize(objective, n_trials=50)

# ✅ 결과 출력
print("Best trial:")
print(study.best_trial.params)
```

    (중략)
    Best trial:
    {'learning_rate': 0.061601661194286, 'depth': 10, 'l2_leaf_reg': 0.5769322320077344, 'bagging_temperature': 0.05128621130619537, 'random_strength': 0.0028870266156700943, 'border_count': 210, 'min_data_in_leaf': 3, 'grow_policy': 'Depthwise'}

```python 

# ✅ best_params 가져오기
best_params = study.best_trial.params.copy()

# ✅ 추가로 필요한 고정 파라미터 넣기
best_params.update({
    'iterations': 3000,
    'loss_function': 'MAE',
    'random_state': 42
})

# ✅ 모델 저장용 리스트
models = []

# ✅ 5-Fold 교차검증
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# ✅ 예측값 저장용
val_scores = []

for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
    print(f"🔁 Fold {fold+1}/5")

    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # ✅ Pool 객체 정의
    train_pool = Pool(X_train, y_train)
    val_pool = Pool(X_val, y_val)

    # ✅ 모델 정의
    model = CatBoostRegressor(**best_params)
    model.fit(train_pool, eval_set=val_pool, use_best_model=True)

    # ✅ 모델 저장
    models.append(model)

    # ✅ 검증 점수 저장 (MAE 기준)
    val_pred = model.predict(val_pool)
    score = mean_absolute_error(y_val, val_pred)
    print(f"✅ Fold {fold+1} MAE: {score:.5f}")
    val_scores.append(score)

# ✅ 전체 평균 점수 출력
print(f"\n📊 평균 MAE (튜닝된 CatBoost): {np.mean(val_scores):.5f}")
```
    (중략)
    📊 평균 MAE (튜닝된 CatBoost): 0.19853

## 4차 Hyper Parameters Review

```
✅ 2차 Hyper Parameters 결과가 3차 결과보다 좋아서 2차 내용을 copy 했습니다. 
✅ 2차와 4차의 외부 차이점은 다양한 Feature을 추가하고, 상장여부, 인수여부 columns을 삭제했습니다. 
✅ 2차 Hyper Parameters 와 차이점 입니다. 
    ◾ 교차검증을 하는 과정에서 
        'early_stopping_rounds': 50     # 성능이 좋아지지 않으면 멈춥니다. 
        'verbose': 100                  # 로그 출력을 100회에 1번씩 합니다. 
        을 삭제했습니다. 

✅ 평균 MAE는 좋아졌습니다. 하지만 Feature_Engineering의 의미가 더 큰 것 같습니다. 
```

# 5차 Hyper Parameters 

```python 
# ✅ Feature, Target 정의
X = train.drop(columns=['성공확률'])
y = train['성공확률']

# ✅ best_params 가져오기 (복사 후 수정 권장)
best_params = study.best_trial.params.copy()

# ✅ 추가로 필요한 고정 파라미터 넣기
best_params.update({
    'iterations': 3000,
    'loss_function': 'MAE',
    'random_state': 42
})

# ✅ 모델 저장용 리스트
models = []

# ✅ 10-Fold 교차검증
kf = KFold(n_splits=10, shuffle=True, random_state=42)

# ✅ 예측값 저장용
val_scores = []

for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
    print(f"🔁 Fold {fold+1}/10")

    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # ✅ Pool 객체 정의
    train_pool = Pool(X_train, y_train)
    val_pool = Pool(X_val, y_val)

    # ✅ 모델 정의
    model = CatBoostRegressor(**best_params)
    model.fit(train_pool, eval_set=val_pool, use_best_model=True)

    # ✅ 모델 저장
    models.append(model)

    # ✅ 검증 점수 저장 (MAE 기준)
    val_pred = model.predict(val_pool)
    score = mean_absolute_error(y_val, val_pred)
    print(f"✅ Fold {fold+1} MAE: {score:.5f}")
    val_scores.append(score)

# ✅ 전체 평균 점수 출력
print(f"\n📊 평균 MAE (튜닝된 CatBoost): {np.mean(val_scores):.5f}")
```

    (중략)
    📊 평균 MAE (튜닝된 CatBoost): 0.19867

## 5차 Hyper Parameters Review

```
✅ k-fold 5번 한 결과보다 좋지 않아서 5번으로 결정했습니다. 
```

# 6차 Hyper Parameters 

```python
# ✅ 튜닝 시작
sampler = optuna.samplers.TPESampler(seed=42)
study = optuna.create_study(direction='minimize', sampler=sampler)
study.optimize(objective, n_trials=100)
```

## 6차 Hyper Parameters Review

    ✅ 4차와 다른 내용은 전부 일치하며, n_trials 를 50에서 100으로 증가시켰습니다. 
    📊 평균 MAE (튜닝된 CatBoost): 0.19780

# 7차 Hyper Parameters 

```python
# ✅ Feature, Target 정의
X = train.drop(columns=['성공확률'])
y = train['성공확률']

# ✅ Optuna objective 함수 정의
def objective(trial):
    params = {
        'iterations': 3000,
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'depth': trial.suggest_int('depth', 4, 10),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),
        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),
        'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0, log=True),
        'border_count': trial.suggest_int('border_count', 64, 254),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 20),
        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),
        'loss_function': 'MAE',
        'early_stopping_rounds': 100,
        'verbose': 0,
        'random_state': 42
    }

    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    mae_list = []

    for train_idx, valid_idx in kf.split(X):
        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

        train_pool = Pool(X_train, y_train)
        valid_pool = Pool(X_valid, y_valid)

        model = CatBoostRegressor(**params)
        model.fit(train_pool, eval_set=valid_pool, use_best_model=True)

        preds = model.predict(valid_pool)
        preds = np.clip(preds, 0, 1)  # ← 이 줄 추가
        mae = mean_absolute_error(y_valid, preds)
        mae_list.append(mae)

    return np.mean(mae_list)

# ✅ 튜닝 시작
sampler = optuna.samplers.TPESampler(seed=42)
study = optuna.create_study(direction='minimize', sampler=sampler)
study.optimize(objective, n_trials=100)

# ✅ 결과 출력
print("Best trial:")
print(study.best_trial.params)
```

    (중략)
    Best trial:
    {'learning_rate': 0.02528730846845928, 'depth': 10, 'l2_leaf_reg': 0.1589791488312824, 'bagging_temperature': 0.054469271620564606, 'random_strength': 0.002793402531131714, 'border_count': 242, 'min_data_in_leaf': 1, 'grow_policy': 'Depthwise'}

```python
# ✅ Feature, Target 정의
X = train.drop(columns=['성공확률'])
y = train['성공확률']

# ✅ best_params 가져오기 (복사 후 수정 권장)
best_params = study.best_trial.params.copy()

# ✅ 추가로 필요한 고정 파라미터 넣기
best_params.update({
    'iterations': 3000,
    'loss_function': 'MAE',
    'random_state': 42
})

# ✅ 모델 저장용 리스트
models = []

# ✅ 10-Fold 교차검증
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# ✅ 예측값 저장용
val_scores = []

for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
    print(f"🔁 Fold {fold+1}/5")

    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # ✅ Pool 객체 정의
    train_pool = Pool(X_train, y_train)
    val_pool = Pool(X_val, y_val)

    # ✅ 모델 정의
    model = CatBoostRegressor(**best_params)
    model.fit(train_pool, eval_set=val_pool, use_best_model=True)

    # ✅ 모델 저장
    models.append(model)

    # ✅ 검증 점수 저장 (MAE 기준)
    val_pred = model.predict(val_pool)
    val_pred = np.clip(val_pred, 0, 1)
    score = mean_absolute_error(y_val, val_pred)
    print(f"✅ Fold {fold+1} MAE: {score:.5f}")
    val_scores.append(score)

# ✅ 전체 평균 점수 출력
print(f"\n📊 평균 MAE (튜닝된 CatBoost): {np.mean(val_scores):.5f}")
```

    (중략)
    📊 평균 MAE (튜닝된 CatBoost): 0.19780

```python 
# 모델의 예측 결과 저장용
predictions = []

for model in models:
    preds = model.predict(X_test)
    predictions.append(preds)

# 모델들의 평균 예측
final_prediction = np.mean(predictions, axis=0)

# 최종 예측값 클리핑 (0~1 범위로 제한)
final_prediction = np.clip(final_prediction, 0, 1)
```

## 7차 Hyper Parameters Review

```
✅ Clipping 
    ◾ Optuna 튜닝 중 :  잘못 튀는 값을 막아서 튜닝이 더 정확해짐
    ◾ 교차검증 중 :      Fold별 MAE 계산할 때, 실제 target과 비교가 더 자연스러움 
    ◾ 최종 예측 :       비현실적인 예측값 (예 : -0.1, 1.2) 이 사라짐
```

# Results 

📌 기본 Parameters 제출 점수 : 0.2244221979

📌 1차 Hyper Parameters 제출 점수 : 0.2213279284

    [1차 수정 Hyper Parameters]

📌 2차 Hyper Parameters 제출 점수 : 0.2176246835	

    [Feature Engineering]

    [2차 수정 Hyper Parameters]

📌 3차 Hyper Parameters 제출 점수 : 0.2190446079

    [Feature Engineering]

    [3차 수정 Hyper Parameters]

📌 4차 Hyper Parameters 제출 점수 : 0.2128477211	

    [Feature Engineering]

    ['인수여부', '상장여부' columns 제거]

    [2차 수정 Hyper Parameters]

📌 5차 Hyper Parameters 제출 점수 : 0.214333596

    [Feature Engineering]

    ['인수여부', '상장여부' columns 제거]

    [4차 수정 Hyper Parameters]

📌 6차 Hyper Parameters 제출 점수 : 0.2123562664

    [Feature Engineering]

    ['인수여부', '상장여부' columns 제거]

    [5차 수정 Hyper Parameters]

📌 7차 Hyper Parameters 제출 점수 : 0.2123562664

    [Feature Engineering]

    ['인수여부', '상장여부' columns 제거]

    [6차 수정 Hyper Parameters]
    