---
title: "[Dacon ê¸°ì—… ì„±ê³µ ì˜ˆì¸¡] Catboost Hyper Parameters"
categories: Model_Training&Tuning
tags: [python, Dacon, Catboost, Hyper_Parameters]
---

# Dacon ê¸°ì—… ì„±ê³µ ì˜ˆì¸¡ Hyper Parameters 

---

# Hyper Parametesr ì´ì „ 

```python
# âœ… Feature, Target ì •ì˜
X = train.drop(columns=['ì„±ê³µí™•ë¥ '])
y = train['ì„±ê³µí™•ë¥ ']

# âœ… ëª¨ë¸ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
models = []

# âœ… 5-Fold êµì°¨ê²€ì¦
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# âœ… ì˜ˆì¸¡ê°’ ì €ì¥ìš©
val_scores = []

for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
    print(f"ğŸ” Fold {fold+1}/5")

    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # âœ… CatBoostRegressor ìƒì„±
    model = CatBoostRegressor(
        iterations=1000,
        learning_rate=0.05,
        depth=6,
        loss_function='MAE',
        early_stopping_rounds=50,
        verbose=100,
        random_state=42
    )

    # âœ… í•™ìŠµ
    model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)

    # âœ… ëª¨ë¸ ì €ì¥
    models.append(model)

    # âœ… ê²€ì¦ ì ìˆ˜ ì €ì¥ (MAE ê¸°ì¤€)
    val_pred = model.predict(X_val)
    score = mean_absolute_error(y_val, val_pred)
    print(f"âœ… Fold {fold+1} MAE: {score:.5f}")
    val_scores.append(score)

# âœ… ì „ì²´ í‰ê·  ì ìˆ˜ ì¶œë ¥
print(f"\nğŸ“Š í‰ê·  MAE: {np.mean(val_scores):.5f}")
```

    (ì¤‘ëµ)
    ğŸ“Š í‰ê·  MAE: 0.20293

## Hyper Parameters ì´ì „ Review

```
âœ… Hyper Parameter í•˜ê¸° ì „ ê¸°ë¡ì…ë‹ˆë‹¤. 
    â—¾ iterations = 1000         # ìµœëŒ€ 1000ë²ˆ í•™ìŠµ
    â—¾ learning_rate = 0.05      # í•™ìŠµë¥ 
    â—¾ depth = 6                 # íŠ¸ë¦¬ ê¹Šì´
    â—¾ loss_function = 'MAE'     # Mean Absolute Error ê¸°ì¤€ìœ¼ë¡œ í•™ìŠµ
    â—¾ early_stopping_round = 50 # 50ë²ˆ ë™ì•ˆ ê°œì„ ì´ ì—†ìœ¼ë©´ í•™ìŠµ ì¤‘ë‹¨
    â—¾ verbose = 100             # 100íšŒë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
    â—¾ random_state = 42         # ì¬í˜„ì„±ì„ ìœ„í•´ ëœë¤ ì‹œë“œ ê³ ì •ì •

âœ… ê° Foldë³„ MAE ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³  í‰ê· ì„ êµ¬í–ˆìŠµë‹ˆë‹¤. 
ğŸ“Š í‰ê·  MAE: 0.20293
```

# 1ì°¨ Hyper Parameters

```python 
# âœ… Feature, Target ì •ì˜
X = train.drop(columns=['ì„±ê³µí™•ë¥ '])
y = train['ì„±ê³µí™•ë¥ ']

# âœ… Optuna objective í•¨ìˆ˜ ì •ì˜
def objective(trial):
    params = {
        'iterations': 1000,
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'depth': trial.suggest_int('depth', 4, 10),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),
        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),
        'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0, log=True),
        'loss_function': 'MAE',
        'early_stopping_rounds': 50,
        'verbose': 0,
        'random_state': 42
    }

    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    mae_list = []

    for train_idx, valid_idx in kf.split(X):
        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

        train_pool = Pool(X_train, y_train)
        valid_pool = Pool(X_valid, y_valid)

        model = CatBoostRegressor(**params)
        model.fit(train_pool, eval_set=valid_pool, use_best_model=True)

        preds = model.predict(valid_pool)
        mae = mean_absolute_error(y_valid, preds)
        mae_list.append(mae)

    return np.mean(mae_list)

# âœ… íŠœë‹ ì‹œì‘
sampler = optuna.samplers.TPESampler(seed=42)
study = optuna.create_study(direction='minimize', sampler=sampler)
study.optimize(objective, n_trials=50)

# âœ… ê²°ê³¼ ì¶œë ¥
print("Best trial:")
print(study.best_trial.params)
```

    (ì¤‘ëµ)
    Best trial:
    {'learning_rate': 0.03249364993947322, 'depth': 8, 'l2_leaf_reg': 4.26944855321362, 'bagging_temperature': 0.12969514204225474, 'random_strength': 0.18317580018607582}

```python 
best_params = study.best_trial.params.copy()

# âœ… 
best_params.update({
    'iterations': 1000,
    'loss_function': 'MAE',
    'early_stopping_rounds': 50,
    'verbose': 100,
    'random_state': 42
})

# âœ… ëª¨ë¸ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
models = []

# âœ… 5-Fold êµì°¨ê²€ì¦
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# âœ… ì˜ˆì¸¡ê°’ ì €ì¥ìš©
val_scores = []

for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
    print(f"ğŸ” Fold {fold+1}/5")

    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # âœ… Pool ê°ì²´ ì •ì˜
    train_pool = Pool(X_train, y_train)
    val_pool = Pool(X_val, y_val)

    # âœ… ëª¨ë¸ ì •ì˜
    model = CatBoostRegressor(**best_params)
    model.fit(train_pool, eval_set=val_pool, use_best_model=True)

    # âœ… ëª¨ë¸ ì €ì¥
    models.append(model)

    # âœ… ê²€ì¦ ì ìˆ˜ ì €ì¥ (MAE ê¸°ì¤€)
    val_pred = model.predict(val_pool)
    score = mean_absolute_error(y_val, val_pred)
    print(f"âœ… Fold {fold+1} MAE: {score:.5f}")
    val_scores.append(score)

# âœ… ì „ì²´ í‰ê·  ì ìˆ˜ ì¶œë ¥
print(f"\nğŸ“Š í‰ê·  MAE (íŠœë‹ëœ CatBoost): {np.mean(val_scores):.5f}")
```

    (ì¤‘ëµ)
    ğŸ“Š í‰ê·  MAE (íŠœë‹ëœ CatBoost): 0.20205

## 1ì°¨ Hyper Parameters Review

```
âœ… 1ì°¨ Hyper Parameter ê¸°ë¡ì…ë‹ˆë‹¤. 
    â—¾ Optunaë¡œ ìë™í™”ëœ íŠœë‹ì…ë‹ˆë‹¤. 
    â—¾ 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3)
        â–ª í•™ìŠµë¥ ì„ ë²”ìœ„(0.01 ~ 0.3)ì—ì„œ íŠœë‹í•©ë‹ˆë‹¤. 
        â–ª ë„ˆë¬´ í¬ë©´ -> ë¹ ë¥´ê²Œ ë°°ìš°ì§€ë§Œ ê³¼ì í•© ìœ„í—˜ 
        â–ª ë„ˆë¬´ ì‘ìœ¼ë©´ -> ì²œì²œíˆ ë°°ìš°ì§€ë§Œ ì•ˆì •ì  
        â–ª ê²°ê³¼ : ì•½ 0.0325 (Hyper Parameters ì „ : 0.05)
    â—¾ 'depth': trial.suggest_int('depth', 4, 10)
        â–ª ê° íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ 4ì—ì„œ 10 ì‚¬ì´ì—ì„œ íŠœë‹í•©ë‹ˆë‹¤. 
        â–ª ë„ˆë¬´ í¬ë©´ -> ë³µì¡í•œ íŒ¨í„´ì„ ì˜ í•™ìŠµí•˜ì§€ë§Œ ê³¼ì í•© ìœ„í—˜ 
        â–ª ë„ˆë¬´ ì‘ìœ¼ë©´ -> ë‹¨ìˆœí•œ ëª¨ë¸, ëœ ê³¼ì í•©
        â–ª ê²°ê³¼ : 8ë¡œ ì„ íƒ (Hyper Parameters ì „ : 6)
    â—¾ l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True)
        â–ª L2 ì •ê·œí™” ê°•ë„ (ê³¼ì í•© ë°©ì§€ìš©)ì„ ë²”ìœ„(0.001 ~ 10)ì—ì„œ log scaleë¡œ íŠœë‹í•©ë‹ˆë‹¤. 
        â–ª ë†’ì´ë©´ -> ëª¨ë¸ì„ ëœ ë³µì¡í•˜ê²Œ (ê³¼ì í•© ë°©ì§€)
        â–ª ë‚®ì¶”ë©´ -> ë” ë³µì¡í•˜ê²Œ í•™ìŠµ ê°€ëŠ¥ 
        â–ª ê²°ê³¼ : 4.27ì •ë„ë¡œ ì„ íƒ
    â—¾ 'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0)
        â–ª ë°ì´í„° ìƒ˜í”Œë§ì— randomnessë¥¼ ì£¼ëŠ” ì •ë„ë¡œ ë²”ìœ„ (0.0 ~ 1.0)ì—ì„œ íŠœë‹í•©ë‹ˆë‹¤. 
        â–ª 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ -> ë³´ìˆ˜ì ìœ¼ë¡œ ìƒ˜í”Œë§ (ëœ ë¬´ì‘ìœ„)
        â–ª 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ -> ë¬´ì‘ìœ„ì„±ì´ ì»¤ì§ 
        â–ª ê²°ê³¼ : 0.13ì ë„ë¡œ ì„ íƒ 
    â—¾ 'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0, log=True)
        â–ª íŠ¸ë¦¬ ë¶„í•  ì‹œ ëœë¤ì„± ì¶”ê°€ ê°•ë„ë¡œ ë²”ìœ„ (0.001 ~ 10)ì—ì„œ log scaleë¡œ íŠœë‹í•©ë‹ˆë‹¤.
        â–ª ë†’ìœ¼ë©´ -> ë¶„í• í•  ë•Œ randomnessë¥¼ ë” ë„£ìŒ (ë‹¤ì–‘ì„± ì¦ê°€)
        â–ª ë‚®ìœ¼ë©´ -> ë” ì •ì§í•˜ê²Œ ìµœì  ë¶„í•  ì°¾ìŒ
        â–ª ê²°ê³¼ : íŠœë‹ í›„ 0.18ì •ë„ë¡œ ì„ íƒ 

ğŸ“Š í‰ê·  MAE: 0.20205
âœ… ì†Œí­ ìƒìŠ¹í•œ ëª¨ìŠµì…ë‹ˆë‹¤. 
```

# 2ì°¨ Hyper Parameters

```python 
# âœ… Feature, Target ì •ì˜
X = train.drop(columns=['ì„±ê³µí™•ë¥ '])
y = train['ì„±ê³µí™•ë¥ ']

# âœ… Optuna objective í•¨ìˆ˜ ì •ì˜
def objective(trial):
    params = {
        'iterations': 3000,
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'depth': trial.suggest_int('depth', 4, 10),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),
        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),
        'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0, log=True),
        'border_count': trial.suggest_int('border_count', 64, 254),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 20),
        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),
        'loss_function': 'MAE',
        'early_stopping_rounds': 100,
        'verbose': 0,
        'random_state': 42
    }


    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    mae_list = []

    for train_idx, valid_idx in kf.split(X):
        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

        train_pool = Pool(X_train, y_train)
        valid_pool = Pool(X_valid, y_valid)

        model = CatBoostRegressor(**params)
        model.fit(train_pool, eval_set=valid_pool, use_best_model=True)

        preds = model.predict(valid_pool)
        mae = mean_absolute_error(y_valid, preds)
        mae_list.append(mae)

    return np.mean(mae_list)

# âœ… íŠœë‹ ì‹œì‘
sampler = optuna.samplers.TPESampler(seed=42)
study = optuna.create_study(direction='minimize', sampler=sampler)
study.optimize(objective, n_trials=50)

# âœ… ê²°ê³¼ ì¶œë ¥
print("Best trial:")
print(study.best_trial.params)
```

    (ì¤‘ëµ)
    Best trial:
    {'learning_rate': 0.028334641989823557, 'depth': 10, 'l2_leaf_reg': 0.0010173074173633859, 'bagging_temperature': 0.3787135134452023, 'random_strength': 0.30665794801197754, 'border_count': 218, 'min_data_in_leaf': 12, 'grow_policy': 'Depthwise'}

```python

# âœ… best_params ê°€ì ¸ì˜¤ê¸°
best_params = study.best_trial.params.copy()

# âœ… ì¶”ê°€ë¡œ í•„ìš”í•œ ê³ ì • íŒŒë¼ë¯¸í„° ë„£ê¸°
best_params.update({
    'iterations': 3000,
    'loss_function': 'MAE',
    'early_stopping_rounds': 50,
    'verbose': 100,
    'random_state': 42
})

# âœ… ëª¨ë¸ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
models = []

# âœ… 5-Fold êµì°¨ê²€ì¦
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# âœ… ì˜ˆì¸¡ê°’ ì €ì¥ìš©
val_scores = []

for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
    print(f"ğŸ” Fold {fold+1}/5")

    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # âœ… Pool ê°ì²´ ì •ì˜
    train_pool = Pool(X_train, y_train)
    val_pool = Pool(X_val, y_val)

    # âœ… ëª¨ë¸ ì •ì˜
    model = CatBoostRegressor(**best_params)
    model.fit(train_pool, eval_set=val_pool, use_best_model=True)

    # âœ… ëª¨ë¸ ì €ì¥
    models.append(model)

    # âœ… ê²€ì¦ ì ìˆ˜ ì €ì¥ (MAE ê¸°ì¤€)
    val_pred = model.predict(val_pool)
    score = mean_absolute_error(y_val, val_pred)
    print(f"âœ… Fold {fold+1} MAE: {score:.5f}")
    val_scores.append(score)

# âœ… ì „ì²´ í‰ê·  ì ìˆ˜ ì¶œë ¥
print(f"\nğŸ“Š í‰ê·  MAE (íŠœë‹ëœ CatBoost): {np.mean(val_scores):.5f}")
```

    (ì¤‘ëµ)
    ğŸ“Š í‰ê·  MAE (íŠœë‹ëœ CatBoost): 0.19909

## 2ì°¨ Hyper Parameters Review

```
âœ… 1ì°¨ Hyper Parameters ì™€ ì™¸ë¶€ ì°¨ì´ì ì€ Feature Engineeringì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. 

âœ… 1ì°¨ Hyper Parameters ì™€ ì°¨ì´ì  ì…ë‹ˆë‹¤. 
    â—¾ iterations 1000 -> 3000
        â–ª ë” ë§ì€ íŠ¸ë¦¬ ìƒì„± ê¸°íšŒë¥¼ ì¤˜ì„œ ë³µì¡í•œ íŒ¨í„´ë„ ìºì¹˜í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. 
    â—¾ early_stopping_rounds 50 -> 100
        â–ª í•™ìŠµì´ ì²œì²œíˆ ì¢‹ì•„ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ ì¦ê°€ì‹œí‚µë‹ˆë‹¤. 
    â—¾ border_count ì¶”ê°€ íŠœë‹
        â–ª featureë¥¼ ì–¼ë§ˆë‚˜ fineí•˜ê²Œ ë‚˜ëˆŒì§€ íŠœë‹ (ëª¨ë¸ ì •ë°€ë„ í–¥ìƒ ê°€ëŠ¥)
    â—¾ min_data_in_leaf ì¶”ê°€ íŠœë‹
        â–ª ê³¼ì í•©(overfitting) ì¡°ì ˆ (ìì‚¬ê·€ì— ìµœì†Œ ë°ì´í„° ìˆ˜ ì¡°ì ˆ)
    â—¾ grow_policy ì¶”ê°€ íŠœë‹ 
        â–ª íŠ¸ë¦¬ ìë¼ëŠ” ë°©ì‹ ìì²´ë¥¼ íŠœë‹í•´ì„œ ìµœì ì˜ êµ¬ì¡° ì°¾ê¸° (Depthwise vs Lossgiuide ë“±)
```

# 3ì°¨ Hyper Parameters 

```python
# âœ… Feature, Target ì •ì˜
X = train.drop(columns=['ì„±ê³µí™•ë¥ '])
y = train['ì„±ê³µí™•ë¥ ']

# âœ… Optuna objective í•¨ìˆ˜ ì •ì˜
def objective(trial):
    params = {
        'iterations': 5000,
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
        'depth': trial.suggest_int('depth', 4, 10),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 10.0, log=True),
        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),
        'random_strength': trial.suggest_float('random_strength', 0.01, 10.0, log=True),
        'border_count': trial.suggest_int('border_count', 32, 255),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 32),
        'grow_policy': 'SymmetricTree',
        'boosting_type': trial.suggest_categorical('boosting_type', ['Plain', 'Ordered']),
        'loss_function': 'MAE',
        'early_stopping_rounds': 200,
        'verbose': 0,
        'random_state': 42
    }



    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    mae_list = []

    for train_idx, valid_idx in kf.split(X):
        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

        train_pool = Pool(X_train, y_train)
        valid_pool = Pool(X_valid, y_valid)

        model = CatBoostRegressor(**params)
        model.fit(train_pool, eval_set=valid_pool, use_best_model=True)

        preds = model.predict(valid_pool)
        mae = mean_absolute_error(y_valid, preds)
        mae_list.append(mae)

    return np.mean(mae_list)

# âœ… íŠœë‹ ì‹œì‘
sampler = optuna.samplers.TPESampler(seed=42)
study = optuna.create_study(direction='minimize', sampler=sampler)
study.optimize(objective, n_trials=50)

# âœ… ê²°ê³¼ ì¶œë ¥
print("Best trial:")
print(study.best_trial.params)
```

    (ì¤‘ëµ)
    Best trial:
    {'learning_rate': 0.011049202253484457, 'depth': 9, 'l2_leaf_reg': 1.3199942261535018, 'bagging_temperature': 0.7290071680409873, 'random_strength': 2.059733535743719, 'border_count': 48, 'min_data_in_leaf': 12, 'boosting_type': 'Ordered'}

```python

# âœ… best_params ê°€ì ¸ì˜¤ê¸°
best_params = study.best_trial.params.copy()

# âœ… ì¶”ê°€ë¡œ í•„ìš”í•œ ê³ ì • íŒŒë¼ë¯¸í„° ë„£ê¸°
best_params.update({
    'iterations': 5000,
    'loss_function': 'MAE',
    'early_stopping_rounds': 200,
    'verbose': 100,
    'random_state': 42
})

# âœ… ëª¨ë¸ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
models = []

# âœ… 5-Fold êµì°¨ê²€ì¦
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# âœ… ì˜ˆì¸¡ê°’ ì €ì¥ìš©
val_scores = []

for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
    print(f"ğŸ” Fold {fold+1}/5")

    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # âœ… Pool ê°ì²´ ì •ì˜
    train_pool = Pool(X_train, y_train)
    val_pool = Pool(X_val, y_val)

    # âœ… ëª¨ë¸ ì •ì˜
    model = CatBoostRegressor(**best_params)
    model.fit(train_pool, eval_set=val_pool, use_best_model=True)

    # âœ… ëª¨ë¸ ì €ì¥
    models.append(model)

    # âœ… ê²€ì¦ ì ìˆ˜ ì €ì¥ (MAE ê¸°ì¤€)
    val_pred = model.predict(val_pool)
    score = mean_absolute_error(y_val, val_pred)
    print(f"âœ… Fold {fold+1} MAE: {score:.5f}")
    val_scores.append(score)

# âœ… ì „ì²´ í‰ê·  ì ìˆ˜ ì¶œë ¥
print(f"\nğŸ“Š í‰ê·  MAE (íŠœë‹ëœ CatBoost): {np.mean(val_scores):.5f}")
```

    (ì¤‘ëµ)
    ğŸ“Š í‰ê·  MAE (íŠœë‹ëœ CatBoost): 0.20051

## 3ì°¨ Hyper Parameters Review

```
âœ… 2ì°¨ Hyper Parameters ì™€ ì°¨ì´ì  ì…ë‹ˆë‹¤. 
    â—¾ iterations 3000 -> 5000 
        â–ª ë” ë§ì€ í•™ìŠµì„ ì‹œë„í•˜ë ¤ê³  í•¨ 
    â—¾ learning_rate 0.3 -> 0.2 
        â–ª ìµœëŒ€ê°’ì„ ì¤„ì—¬ ë” ì„¸ë°€í•œ í•™ìŠµì„ ìœ ë„ 
    â—¾ l2_leaf_reg 1e-3 -> 0.01 
        â–ª ìµœì†Œê°’ì„ ëŠ˜ë ¤ ë” ê°•í•œ ê·œì œë¥¼ ì‚¬ìš©. ê³¼ì í•©ì„ ë°©ì§€í•˜ë ¤ëŠ” ì˜ë„ 
    â—¾ random_strength 1e-3 -> 0.01
        â–ª ëª¨ë¸ì— ë” ë§ì€ ëœë¤ì„±ì„ ì¶”ê°€ 
    â—¾ border_count 64 ~ 254 ì—ì„œ 32 ~ 255 ë²”ì£¼ ë³€ê²½
        â–ª ë‹¨ìˆœí™”ëœ binningì„ ì‚¬ìš© 
    â—¾ min_data_in_leaf 20 -> 32
        â–ª ìµœëŒ€ê°’ì„ í™•ëŒ€í•˜ë©° ë” ë„’ì€ ë²”ìœ„ì—ì„œ ì¡°ì • ê°€ëŠ¥
    â—¾ grow_policy ë‹¤ì–‘í•œ ì˜µì…˜ (SymmetricTree, Depthwise, Lossguide)ì—ì„œ SymmetricTreeë¡œ ê³ ì •
        â–ª ê³¼ì í•© ë°©ì§€
    â—¾ early_stopping_rounds 100 -> 200
        â–ª ì¡°ê¸° ì¢…ë£Œ ê¸°ì¤€ì„ ê°•í™” 
```

# 4ì°¨ Hyper Parameters 

```python 
# âœ… Feature, Target ì •ì˜
X = train.drop(columns=['ì„±ê³µí™•ë¥ '])
y = train['ì„±ê³µí™•ë¥ ']

# âœ… Optuna objective í•¨ìˆ˜ ì •ì˜
def objective(trial):
    params = {
        'iterations': 3000,
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'depth': trial.suggest_int('depth', 4, 10),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),
        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),
        'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0, log=True),
        'border_count': trial.suggest_int('border_count', 64, 254),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 20),
        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),
        'loss_function': 'MAE',
        'early_stopping_rounds': 100,
        'verbose': 0,
        'random_state': 42
    }


    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    mae_list = []

    for train_idx, valid_idx in kf.split(X):
        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

        train_pool = Pool(X_train, y_train)
        valid_pool = Pool(X_valid, y_valid)

        model = CatBoostRegressor(**params)
        model.fit(train_pool, eval_set=valid_pool, use_best_model=True)

        preds = model.predict(valid_pool)
        mae = mean_absolute_error(y_valid, preds)
        mae_list.append(mae)

    return np.mean(mae_list)

# âœ… íŠœë‹ ì‹œì‘
sampler = optuna.samplers.TPESampler(seed=42)
study = optuna.create_study(direction='minimize', sampler=sampler)
study.optimize(objective, n_trials=50)

# âœ… ê²°ê³¼ ì¶œë ¥
print("Best trial:")
print(study.best_trial.params)
```

    (ì¤‘ëµ)
    Best trial:
    {'learning_rate': 0.061601661194286, 'depth': 10, 'l2_leaf_reg': 0.5769322320077344, 'bagging_temperature': 0.05128621130619537, 'random_strength': 0.0028870266156700943, 'border_count': 210, 'min_data_in_leaf': 3, 'grow_policy': 'Depthwise'}

```python 

# âœ… best_params ê°€ì ¸ì˜¤ê¸°
best_params = study.best_trial.params.copy()

# âœ… ì¶”ê°€ë¡œ í•„ìš”í•œ ê³ ì • íŒŒë¼ë¯¸í„° ë„£ê¸°
best_params.update({
    'iterations': 3000,
    'loss_function': 'MAE',
    'random_state': 42
})

# âœ… ëª¨ë¸ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
models = []

# âœ… 5-Fold êµì°¨ê²€ì¦
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# âœ… ì˜ˆì¸¡ê°’ ì €ì¥ìš©
val_scores = []

for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
    print(f"ğŸ” Fold {fold+1}/5")

    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # âœ… Pool ê°ì²´ ì •ì˜
    train_pool = Pool(X_train, y_train)
    val_pool = Pool(X_val, y_val)

    # âœ… ëª¨ë¸ ì •ì˜
    model = CatBoostRegressor(**best_params)
    model.fit(train_pool, eval_set=val_pool, use_best_model=True)

    # âœ… ëª¨ë¸ ì €ì¥
    models.append(model)

    # âœ… ê²€ì¦ ì ìˆ˜ ì €ì¥ (MAE ê¸°ì¤€)
    val_pred = model.predict(val_pool)
    score = mean_absolute_error(y_val, val_pred)
    print(f"âœ… Fold {fold+1} MAE: {score:.5f}")
    val_scores.append(score)

# âœ… ì „ì²´ í‰ê·  ì ìˆ˜ ì¶œë ¥
print(f"\nğŸ“Š í‰ê·  MAE (íŠœë‹ëœ CatBoost): {np.mean(val_scores):.5f}")
```
    (ì¤‘ëµ)
    ğŸ“Š í‰ê·  MAE (íŠœë‹ëœ CatBoost): 0.19853

## 4ì°¨ Hyper Parameters Review

```
âœ… 2ì°¨ Hyper Parameters ê²°ê³¼ê°€ 3ì°¨ ê²°ê³¼ë³´ë‹¤ ì¢‹ì•„ì„œ 2ì°¨ ë‚´ìš©ì„ copy í–ˆìŠµë‹ˆë‹¤. 
âœ… 2ì°¨ì™€ 4ì°¨ì˜ ì™¸ë¶€ ì°¨ì´ì ì€ ë‹¤ì–‘í•œ Featureì„ ì¶”ê°€í•˜ê³ , ìƒì¥ì—¬ë¶€, ì¸ìˆ˜ì—¬ë¶€ columnsì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤. 
âœ… 2ì°¨ Hyper Parameters ì™€ ì°¨ì´ì  ì…ë‹ˆë‹¤. 
    â—¾ êµì°¨ê²€ì¦ì„ í•˜ëŠ” ê³¼ì •ì—ì„œ 
        'early_stopping_rounds': 50     # ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ì§€ ì•Šìœ¼ë©´ ë©ˆì¶¥ë‹ˆë‹¤. 
        'verbose': 100                  # ë¡œê·¸ ì¶œë ¥ì„ 100íšŒì— 1ë²ˆì”© í•©ë‹ˆë‹¤. 
        ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤. 

âœ… í‰ê·  MAEëŠ” ì¢‹ì•„ì¡ŒìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ Feature_Engineeringì˜ ì˜ë¯¸ê°€ ë” í° ê²ƒ ê°™ìŠµë‹ˆë‹¤. 
```

# 5ì°¨ Hyper Parameters 

```python 
# âœ… Feature, Target ì •ì˜
X = train.drop(columns=['ì„±ê³µí™•ë¥ '])
y = train['ì„±ê³µí™•ë¥ ']

# âœ… best_params ê°€ì ¸ì˜¤ê¸° (ë³µì‚¬ í›„ ìˆ˜ì • ê¶Œì¥)
best_params = study.best_trial.params.copy()

# âœ… ì¶”ê°€ë¡œ í•„ìš”í•œ ê³ ì • íŒŒë¼ë¯¸í„° ë„£ê¸°
best_params.update({
    'iterations': 3000,
    'loss_function': 'MAE',
    'random_state': 42
})

# âœ… ëª¨ë¸ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
models = []

# âœ… 10-Fold êµì°¨ê²€ì¦
kf = KFold(n_splits=10, shuffle=True, random_state=42)

# âœ… ì˜ˆì¸¡ê°’ ì €ì¥ìš©
val_scores = []

for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
    print(f"ğŸ” Fold {fold+1}/10")

    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # âœ… Pool ê°ì²´ ì •ì˜
    train_pool = Pool(X_train, y_train)
    val_pool = Pool(X_val, y_val)

    # âœ… ëª¨ë¸ ì •ì˜
    model = CatBoostRegressor(**best_params)
    model.fit(train_pool, eval_set=val_pool, use_best_model=True)

    # âœ… ëª¨ë¸ ì €ì¥
    models.append(model)

    # âœ… ê²€ì¦ ì ìˆ˜ ì €ì¥ (MAE ê¸°ì¤€)
    val_pred = model.predict(val_pool)
    score = mean_absolute_error(y_val, val_pred)
    print(f"âœ… Fold {fold+1} MAE: {score:.5f}")
    val_scores.append(score)

# âœ… ì „ì²´ í‰ê·  ì ìˆ˜ ì¶œë ¥
print(f"\nğŸ“Š í‰ê·  MAE (íŠœë‹ëœ CatBoost): {np.mean(val_scores):.5f}")
```

    (ì¤‘ëµ)
    ğŸ“Š í‰ê·  MAE (íŠœë‹ëœ CatBoost): 0.19867

## 5ì°¨ Hyper Parameters Review

```
âœ… k-fold 5ë²ˆ í•œ ê²°ê³¼ë³´ë‹¤ ì¢‹ì§€ ì•Šì•„ì„œ 5ë²ˆìœ¼ë¡œ ê²°ì •í–ˆìŠµë‹ˆë‹¤. 
```

# 6ì°¨ Hyper Parameters 

```python
# âœ… íŠœë‹ ì‹œì‘
sampler = optuna.samplers.TPESampler(seed=42)
study = optuna.create_study(direction='minimize', sampler=sampler)
study.optimize(objective, n_trials=100)
```

## 6ì°¨ Hyper Parameters Review

    âœ… 4ì°¨ì™€ ë‹¤ë¥¸ ë‚´ìš©ì€ ì „ë¶€ ì¼ì¹˜í•˜ë©°, n_trials ë¥¼ 50ì—ì„œ 100ìœ¼ë¡œ ì¦ê°€ì‹œì¼°ìŠµë‹ˆë‹¤. 
    ğŸ“Š í‰ê·  MAE (íŠœë‹ëœ CatBoost): 0.19780

# 7ì°¨ Hyper Parameters 

```python
# âœ… Feature, Target ì •ì˜
X = train.drop(columns=['ì„±ê³µí™•ë¥ '])
y = train['ì„±ê³µí™•ë¥ ']

# âœ… Optuna objective í•¨ìˆ˜ ì •ì˜
def objective(trial):
    params = {
        'iterations': 3000,
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'depth': trial.suggest_int('depth', 4, 10),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),
        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),
        'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0, log=True),
        'border_count': trial.suggest_int('border_count', 64, 254),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 20),
        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),
        'loss_function': 'MAE',
        'early_stopping_rounds': 100,
        'verbose': 0,
        'random_state': 42
    }

    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    mae_list = []

    for train_idx, valid_idx in kf.split(X):
        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

        train_pool = Pool(X_train, y_train)
        valid_pool = Pool(X_valid, y_valid)

        model = CatBoostRegressor(**params)
        model.fit(train_pool, eval_set=valid_pool, use_best_model=True)

        preds = model.predict(valid_pool)
        preds = np.clip(preds, 0, 1)  # â† ì´ ì¤„ ì¶”ê°€
        mae = mean_absolute_error(y_valid, preds)
        mae_list.append(mae)

    return np.mean(mae_list)

# âœ… íŠœë‹ ì‹œì‘
sampler = optuna.samplers.TPESampler(seed=42)
study = optuna.create_study(direction='minimize', sampler=sampler)
study.optimize(objective, n_trials=100)

# âœ… ê²°ê³¼ ì¶œë ¥
print("Best trial:")
print(study.best_trial.params)
```

    (ì¤‘ëµ)
    Best trial:
    {'learning_rate': 0.02528730846845928, 'depth': 10, 'l2_leaf_reg': 0.1589791488312824, 'bagging_temperature': 0.054469271620564606, 'random_strength': 0.002793402531131714, 'border_count': 242, 'min_data_in_leaf': 1, 'grow_policy': 'Depthwise'}

```python
# âœ… Feature, Target ì •ì˜
X = train.drop(columns=['ì„±ê³µí™•ë¥ '])
y = train['ì„±ê³µí™•ë¥ ']

# âœ… best_params ê°€ì ¸ì˜¤ê¸° (ë³µì‚¬ í›„ ìˆ˜ì • ê¶Œì¥)
best_params = study.best_trial.params.copy()

# âœ… ì¶”ê°€ë¡œ í•„ìš”í•œ ê³ ì • íŒŒë¼ë¯¸í„° ë„£ê¸°
best_params.update({
    'iterations': 3000,
    'loss_function': 'MAE',
    'random_state': 42
})

# âœ… ëª¨ë¸ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
models = []

# âœ… 10-Fold êµì°¨ê²€ì¦
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# âœ… ì˜ˆì¸¡ê°’ ì €ì¥ìš©
val_scores = []

for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
    print(f"ğŸ” Fold {fold+1}/5")

    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # âœ… Pool ê°ì²´ ì •ì˜
    train_pool = Pool(X_train, y_train)
    val_pool = Pool(X_val, y_val)

    # âœ… ëª¨ë¸ ì •ì˜
    model = CatBoostRegressor(**best_params)
    model.fit(train_pool, eval_set=val_pool, use_best_model=True)

    # âœ… ëª¨ë¸ ì €ì¥
    models.append(model)

    # âœ… ê²€ì¦ ì ìˆ˜ ì €ì¥ (MAE ê¸°ì¤€)
    val_pred = model.predict(val_pool)
    val_pred = np.clip(val_pred, 0, 1)
    score = mean_absolute_error(y_val, val_pred)
    print(f"âœ… Fold {fold+1} MAE: {score:.5f}")
    val_scores.append(score)

# âœ… ì „ì²´ í‰ê·  ì ìˆ˜ ì¶œë ¥
print(f"\nğŸ“Š í‰ê·  MAE (íŠœë‹ëœ CatBoost): {np.mean(val_scores):.5f}")
```

    (ì¤‘ëµ)
    ğŸ“Š í‰ê·  MAE (íŠœë‹ëœ CatBoost): 0.19780

```python 
# ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ìš©
predictions = []

for model in models:
    preds = model.predict(X_test)
    predictions.append(preds)

# ëª¨ë¸ë“¤ì˜ í‰ê·  ì˜ˆì¸¡
final_prediction = np.mean(predictions, axis=0)

# ìµœì¢… ì˜ˆì¸¡ê°’ í´ë¦¬í•‘ (0~1 ë²”ìœ„ë¡œ ì œí•œ)
final_prediction = np.clip(final_prediction, 0, 1)
```

## 7ì°¨ Hyper Parameters Review

```
âœ… Clipping 
    â—¾ Optuna íŠœë‹ ì¤‘ :  ì˜ëª» íŠ€ëŠ” ê°’ì„ ë§‰ì•„ì„œ íŠœë‹ì´ ë” ì •í™•í•´ì§
    â—¾ êµì°¨ê²€ì¦ ì¤‘ :      Foldë³„ MAE ê³„ì‚°í•  ë•Œ, ì‹¤ì œ targetê³¼ ë¹„êµê°€ ë” ìì—°ìŠ¤ëŸ¬ì›€ 
    â—¾ ìµœì¢… ì˜ˆì¸¡ :       ë¹„í˜„ì‹¤ì ì¸ ì˜ˆì¸¡ê°’ (ì˜ˆ : -0.1, 1.2) ì´ ì‚¬ë¼ì§
```

# Results 

ğŸ“Œ ê¸°ë³¸ Parameters ì œì¶œ ì ìˆ˜ : 0.2244221979

ğŸ“Œ 1ì°¨ Hyper Parameters ì œì¶œ ì ìˆ˜ : 0.2213279284

    [1ì°¨ ìˆ˜ì • Hyper Parameters]

ğŸ“Œ 2ì°¨ Hyper Parameters ì œì¶œ ì ìˆ˜ : 0.2176246835	

    [Feature Engineering]

    [2ì°¨ ìˆ˜ì • Hyper Parameters]

ğŸ“Œ 3ì°¨ Hyper Parameters ì œì¶œ ì ìˆ˜ : 0.2190446079

    [Feature Engineering]

    [3ì°¨ ìˆ˜ì • Hyper Parameters]

ğŸ“Œ 4ì°¨ Hyper Parameters ì œì¶œ ì ìˆ˜ : 0.2128477211	

    [Feature Engineering]

    ['ì¸ìˆ˜ì—¬ë¶€', 'ìƒì¥ì—¬ë¶€' columns ì œê±°]

    [2ì°¨ ìˆ˜ì • Hyper Parameters]

ğŸ“Œ 5ì°¨ Hyper Parameters ì œì¶œ ì ìˆ˜ : 0.214333596

    [Feature Engineering]

    ['ì¸ìˆ˜ì—¬ë¶€', 'ìƒì¥ì—¬ë¶€' columns ì œê±°]

    [4ì°¨ ìˆ˜ì • Hyper Parameters]

ğŸ“Œ 6ì°¨ Hyper Parameters ì œì¶œ ì ìˆ˜ : 0.2123562664

    [Feature Engineering]

    ['ì¸ìˆ˜ì—¬ë¶€', 'ìƒì¥ì—¬ë¶€' columns ì œê±°]

    [5ì°¨ ìˆ˜ì • Hyper Parameters]

ğŸ“Œ 7ì°¨ Hyper Parameters ì œì¶œ ì ìˆ˜ : 0.2123562664

    [Feature Engineering]

    ['ì¸ìˆ˜ì—¬ë¶€', 'ìƒì¥ì—¬ë¶€' columns ì œê±°]

    [6ì°¨ ìˆ˜ì • Hyper Parameters]
    