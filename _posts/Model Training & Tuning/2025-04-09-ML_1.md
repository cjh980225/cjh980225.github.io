---
title: "[Dacon ê¸°ì—… ì„±ê³µ ì˜ˆì¸¡] LGBMìœ¼ë¡œ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ë§Œë“¤ê¸° & ì „ì²˜ë¦¬ ì‹¤í—˜ ê¸°ë¡"
categories: Model_Training&Tuning
tags: [python, LGBM, KFold, Dacon, Ensemble]
---

# Dacon ê¸°ì—… ì„±ê³µ ì˜ˆì¸¡

# Import

---

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import KFold
from lightgbm import LGBMRegressor, early_stopping, log_evaluation
```

---

â—¾ pandas

    â–ª ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ë‹¤ë£¨ê¸° ìœ„í•œ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ 
    â–ª csv íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°, ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ì»¬ëŸ¼ ì„ íƒ ë“± ê¸°ë³¸ì ì¸ ë°ì´í„° í•¸ë“¤ë§ì— ì‚¬ìš©ë¨ 
    
â—¾ numpy 

    â–ª ìˆ˜ì¹˜ ê³„ì‚°ì— íŠ¹í™”ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ 
    â–ª í‰ê· , ë°°ì—´ ì—°ì‚°, ì˜ˆì¸¡ê°’ í‰ê·  ê³„ì‚° ë“±ì— ì‚¬ìš©ë¨
        â–« ì˜ˆ - np.mean(perdictions, axis = 0)

â—¾ from sklearn.preprocessing import LabelEncoder

    â–ª ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë°”ê¿”ì£¼ëŠ” ì¸ì½”ë”
    â–ª ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ ë¬¸ì ëŒ€ì‹  ìˆ«ìë¥¼ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ ì²˜ë¦¬ 
        â–« ì˜ˆ - êµ­ê°€, íˆ¬ìë‹¨ê³„ ë“±ì„ ìˆ«ìë¡œ ë³€í™˜
        
â—¾ from sklearn.model_selection import KFold

    â–ª K-Fold êµì°¨ê²€ì¦ì„ ìœ„í•œ ë„êµ¬ 
    â–ª ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ê°œì˜ Foldë¡œ ë‚˜ëˆ„ì–´ ëª¨ë¸ì„ í‰ê°€í•  ë•Œ ì‚¬ìš©ë¨
        â–« ì˜ˆ - 5ê°œì˜ Foldë¡œ ë‚˜ëˆ„ì–´ í•™ìŠµ / ê²€ì¦ ë°˜ë³µ
        
â—¾ from lightgbm imort LGBMRegressor, early_stopping, log_evaluation

    â–ª LightGBM ëª¨ë¸ê³¼ í•™ìŠµ ì œì–´ ë„êµ¬
        â–« LGBMRegressor : ë¹ ë¥´ê³  ì„±ëŠ¥ ì¢‹ì€ íšŒê·€ ëª¨ë¸ (íŠ¸ë¦¬ ê¸°ë°˜)
        â–« early_stopping : ì„±ëŠ¥ì´ ë” ì´ìƒ ì¢‹ì•„ì§€ì§€ ì•Šìœ¼ë©´ í•™ìŠµ ë©ˆì¶”ê¸° 
        â–« log_evaluation : í•™ìŠµ ë¡œê·¸ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ì¶œë ¥í•˜ê¸° ìœ„í•œ ì½œë°± 

---

```python
import pandas as pd
import numpy as np
import sklearn
import lightgbm
import sys

print("ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ ëª©ë¡")

print("Python:", sys.version)
print("pandas:", pd.__version__)
print("numpy:", np.__version__)
print("scikit-learn:", sklearn.__version__)
print("lightgbm:", lightgbm.__version__)
```

---

    ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ ëª©ë¡
    Python: 3.13.0 (tags/v3.13.0:60403a5, Oct  7 2024, 09:38:07) [MSC v.1941 64 bit (AMD64)]
    pandas: 2.2.3
    numpy: 2.2.4
    scikit-learn: 1.6.1
    lightgbm: 4.6.0
    

# Data Load

âœ… ë°ì´í„° ë¡œë“œ

---

```python
train = pd.read_csv('Dacon_data/train.csv')
test = pd.read_csv('Dacon_data/test.csv')
sample_submission = pd.read_csv('Dacon_data/sample_submission.csv')
```

---

âœ… train, test columnsì—ì„œ objectì¸ ê²ƒë“¤ì˜ unique()ê°’ í™•ì¸í•˜ê¸°

âœ… train, test columnsì—ì„œ nullê°’ì´ ì¡´ì¬í•˜ëŠ” columns í™•ì¸í•˜ê¸°

---

```python
# train ë°ì´í„° object ì»¬ëŸ¼ ê³ ìœ ê°’ í™•ì¸
for col in train.select_dtypes('object').columns:
    print(f"ğŸ“Œ[Train] {col} :", train[col].unique())

# test ë°ì´í„° object ì»¬ëŸ¼ ê³ ìœ ê°’ í™•ì¸
for col in test.select_dtypes('object').columns:
    print(f"ğŸ”[Test] {col} :", test[col].unique())

# ê²°ì¸¡ì¹˜ í™•ì¸
print("ğŸ“Œ [Train] Null ì²´í¬")
print(train.isnull().sum()[train.isnull().sum() > 0])

print("ğŸ” [Test] Null ì²´í¬")
print(test.isnull().sum()[test.isnull().sum() > 0])
```

---

    ğŸ“Œ[Train] ID : ['TRAIN_0000' 'TRAIN_0001' 'TRAIN_0002' ... 'TRAIN_4373' 'TRAIN_4374'
     'TRAIN_4375']
    ğŸ“Œ[Train] êµ­ê°€ : ['CT005' 'CT006' 'CT007' 'CT002' 'CT008' 'CT010' 'CT001' 'CT009' 'CT003'
     'CT004']
    ğŸ“Œ[Train] ë¶„ì•¼ : ['ì´ì»¤ë¨¸ìŠ¤' 'í•€í…Œí¬' 'ê¸°ìˆ ' nan 'ì—ë“€í…Œí¬' 'ê²Œì„' 'í—¬ìŠ¤ì¼€ì–´' 'ë¬¼ë¥˜' 'í‘¸ë“œí…Œí¬' 'AI' 'ì—ë„ˆì§€']
    ğŸ“Œ[Train] íˆ¬ìë‹¨ê³„ : ['Series A' 'Seed' 'Series C' 'Series B' 'IPO']
    ğŸ“Œ[Train] ì¸ìˆ˜ì—¬ë¶€ : ['No' 'Yes']
    ğŸ“Œ[Train] ìƒì¥ì—¬ë¶€ : ['No' 'Yes']
    ğŸ“Œ[Train] ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›) : [nan '2500-3500' '3500-4500' '1500-2500' '4500-6000' '6000ì´ìƒ']
    ğŸ”[Test] ID : ['TEST_0000' 'TEST_0001' 'TEST_0002' ... 'TEST_1752' 'TEST_1753'
     'TEST_1754']
    ğŸ”[Test] êµ­ê°€ : ['CT010' 'CT001' 'CT006' 'CT003' 'CT005' 'CT009' 'CT004' 'CT007' 'CT002'
     'CT008']
    ğŸ”[Test] ë¶„ì•¼ : ['í•€í…Œí¬' 'í‘¸ë“œí…Œí¬' 'ì—ë“€í…Œí¬' 'ì—ë„ˆì§€' nan 'ê²Œì„' 'ë¬¼ë¥˜' 'ì´ì»¤ë¨¸ìŠ¤' 'AI' 'í—¬ìŠ¤ì¼€ì–´' 'ê¸°ìˆ ']
    ğŸ”[Test] íˆ¬ìë‹¨ê³„ : ['Series C' 'IPO' 'Seed' 'Series A' 'Series B']
    ğŸ”[Test] ì¸ìˆ˜ì—¬ë¶€ : ['No' 'Yes']
    ğŸ”[Test] ìƒì¥ì—¬ë¶€ : ['Yes' 'No']
    ğŸ”[Test] ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›) : ['1500-2500' nan '6000ì´ìƒ' '3500-4500' '4500-6000' '2500-3500']
    ğŸ“Œ [Train] Null ì²´í¬
    ë¶„ì•¼            857
    ì§ì› ìˆ˜          174
    ê³ ê°ìˆ˜(ë°±ë§Œëª…)     1320
    ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)    1220
    dtype: int64
    ğŸ” [Test] Null ì²´í¬
    ë¶„ì•¼           354
    ì§ì› ìˆ˜          76
    ê³ ê°ìˆ˜(ë°±ë§Œëª…)     547
    ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)    487
    dtype: int64
    

â—¾ ëª¨ë¸ì— ë„£ê¸° ì „, Object Typeë“¤ì„ ì²˜ë¦¬í•´ì•¼ í•˜ë¯€ë¡œ ê° ì»¬ëŸ¼ì´ ì–´ë–¤ ê°’ì„ ê°€ì§€ëŠ”ì§€ ë¯¸ë¦¬ í™•ì¸í•´ë‘ëŠ” ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤. 

â—¾ ë˜í•œ ëª¨ë¸ í•™ìŠµì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” ê²°ì¸¡ê°’ì„ ë¨¼ì € ì°¾ì•„ë³´ê³  ì±„ìš°ê±°ë‚˜ ì „ì²˜ë¦¬í•  ì „ëµì„ ì„¸ìš°ê¸° ìœ„í•¨ì…ë‹ˆë‹¤. 

â—¾ trainê³¼ test ëª¨ë‘ ë™ì¼í•œ ì»¬ëŸ¼ì— ê²°ì¸¡ì¹˜ê°€ ì¡´ì¬í•˜ê³  ìˆì—ˆê³ , object íƒ€ì…ì˜ ì»¬ëŸ¼ë„ ë™ì¼í•œ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. 

    â–ª ì‚¬ì „ í™•ì¸ì€ ì „ì²˜ë¦¬ ì‹œ ì¼ê´€ëœ ì²˜ë¦¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•´ì£¼ë©°, ì˜ˆì¸¡ ëª¨ë¸ì˜ ì•ˆì •ì„±ì—ë„ ê¸ì •ì ì¸ ì˜í–¥ì„ ì¤ë‹ˆë‹¤. 

# Data Preprocessing

âœ… ì„¤ë¦½ì—°ë„ â†’ ê¸°ì—…ë‚˜ì´ë¡œ ë³€í™˜

---

```python
# ê¸°ì¤€ ì—°ë„ ì„¤ì •
CURRENT_YEAR = 2025

# ê¸°ì—… ë‚˜ì´ ìƒì„±
train['ê¸°ì—…ë‚˜ì´'] = CURRENT_YEAR - train['ì„¤ë¦½ì—°ë„']
test['ê¸°ì—…ë‚˜ì´'] = CURRENT_YEAR - test['ì„¤ë¦½ì—°ë„']

# ê¸°ì¡´ ì»¬ëŸ¼ ì œê±°
train = train.drop(columns=['ì„¤ë¦½ì—°ë„'])
test = test.drop(columns=['ì„¤ë¦½ì—°ë„'])
```

---

â—¾ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì…ì¥ì—ì„œëŠ” 1999, 2015, 2020 ê°™ì€ ì ˆëŒ€ ì—°ë„ ìˆ«ìëŠ” ì˜ë¯¸ê°€ ì˜ ì „ë‹¬ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 

â—¾ ëŒ€ì‹  'ì„¤ë¦½ëœì§€ 5ë…„ ëœ ê¸°ì—…' ì²˜ëŸ¼ ìƒëŒ€ì ì¸ ìˆ˜ì¹˜ë¥¼ ì£¼ë©´ ëª¨ë¸ì´ ì‹œê°„ íë¦„ì˜ ì˜í–¥ì„ ë” ì˜ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

â—¾ ê·¸ë˜ì„œ ê¸°ì¤€ ì—°ë„ë¥¼ ì„¤ì •í•˜ê³ , ê¸°ì—… ë‚˜ì´ë¥¼ ìƒì„±í•˜ê³  ê¸°ì¡´ì˜ ì»¬ëŸ¼ì„ ì œê±°í•˜ì˜€ìŠµë‹ˆë‹¤. 

---

```python
train = train.drop(columns=['ID'], axis = 1)
test = test.drop(columns=['ID'], axis = 1)

# ìˆ˜ì¹˜í˜• ì²˜ë¦¬
numeric_features = ['ê¸°ì—…ë‚˜ì´', 'ì§ì› ìˆ˜', 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)', 'ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'ì—°ë§¤ì¶œ(ì–µì›)', 'SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)']

# ì´ì§„í˜• ì²˜ë¦¬
bool_features = ['ì¸ìˆ˜ì—¬ë¶€', 'ìƒì¥ì—¬ë¶€']
bool_map = {'Yes': 1, 'No': 0}
for col in bool_features :
    train[col] = train[col].map(bool_map)
    test[col] = test[col].map(bool_map)

# ë²”ì£¼í˜• ì²˜ë¦¬
category_features = ['êµ­ê°€', 'ë¶„ì•¼', 'íˆ¬ìë‹¨ê³„', 'ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)']
encoders = {}

for feature in category_features:
    encoders[feature] = LabelEncoder()
    train[feature] = train[feature].fillna('Missing')
    test[feature] = test[feature].fillna('Missing')
    train[feature] = encoders[feature].fit_transform(train[feature])
    test[feature] = encoders[feature].transform(test[feature])
    
# ê³ ê°ìˆ˜(ë°±ë§Œëª…): ì „ì²´ í‰ê· ìœ¼ë¡œ ì±„ìš°ê¸°
customer_mean = train['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'].mean()
train['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'] = train['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'].fillna(customer_mean)
test['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'] = test['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'].fillna(customer_mean)

# ì§ì› ìˆ˜: êµ­ê°€ë³„ í‰ê· ìœ¼ë¡œ ì±„ìš°ê¸°
train['ì§ì› ìˆ˜'] = train.groupby('êµ­ê°€')['ì§ì› ìˆ˜'].transform(lambda x: x.fillna(x.mean()))

# test ë°ì´í„°ëŠ” êµ­ê°€ë³„ í‰ê· ì„ trainì—ì„œ ê°€ì ¸ì™€ì„œ ì±„ìš°ê¸°
country_mean = train.groupby('êµ­ê°€')['ì§ì› ìˆ˜'].mean()
test['ì§ì› ìˆ˜'] = test.apply(
    lambda row: country_mean[row['êµ­ê°€']] if pd.isnull(row['ì§ì› ìˆ˜']) else row['ì§ì› ìˆ˜'], axis=1
)

# ì»¬ëŸ¼ëª…ì˜ ë„ì–´ì“°ê¸°ë¥¼ ì–¸ë”ë°”ë¡œ ëŒ€ì²´
train.columns = train.columns.str.replace(" ", "_")
test.columns = test.columns.str.replace(" ", "_")
```

---

```python
train = train.drop(columns=['ID'], axis = 1)
test = test.drop(columns=['ID'], axis = 1)
```

â—¾ IDëŠ” ë‹¨ìˆœíˆ ê° ìƒ˜í”Œì„ êµ¬ë¶„í•´ì£¼ëŠ” ê³ ìœ  ë²ˆí˜¸ì¼ ë¿, ì˜ˆì¸¡ ëª¨ë¸ì´ ë„ì›€ì´ ë˜ëŠ” ì •ë³´ëŠ” ì•„ë‹™ë‹ˆë‹¤. 

â—¾ ì˜¤íˆë ¤ í•™ìŠµì— ë¶ˆí•„ìš”í•œ ì¡ìŒì´ ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì œê±°í•©ë‹ˆë‹¤. 

```python
numeric_features = [...]
bool_features = [...]
category_features = [...]
```

â—¾ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•  ë•ŒëŠ” ë³€ìˆ˜ì˜ ì„±ê²©ì— ë”°ë¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ë‹¤ë¤„ì•¼ í•˜ê¸° ë•Œë¬¸ì— featuresë¥¼ í†µí•´ ì„¸ ê°€ì§€ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤. 
    
    â–ª ìˆ˜ì¹˜í˜• ë³€ìˆ˜ (numeric_features) : í‰ê· ì´ë‚˜ í‘œì¤€í¸ì°¨ ë“±ì„ í™œìš©í•œ ì²˜ë¦¬ê°€ ê°€ëŠ¥
    â–ª ì´ì§„í˜• ë³€ìˆ˜ (bool_features) : Yes, No â†’ 1 / 0 ìœ¼ë¡œ ë³€í™˜
    â–ª ë²”ì£¼í˜• ë³€ìˆ˜ (category_features) : ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆê²Œ ìˆ«ìë¡œ ë³€í™˜ í•„ìš”
        
```python
encoders = {}
```

â—¾ ì—¬ëŸ¬ ê°œì˜ ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ì²˜ë¦¬í•  ê±´ë°, ê°ê°ì˜ ë³€ìˆ˜ë§ˆë‹¤ LabelEncoder ê°ì²´ë¥¼ ë”°ë¡œ ë§Œë“¤ì–´ì„œ ì¶”í›„ì— í•„ìš”í•  ë•Œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤. 

```python
train[feature] = train[feature].fillna('Missing')
test[feature] = test[feature].fillna('Missing')
```

â—¾ ë²”ì£¼í˜• ë³€ìˆ˜ëŠ” í‰ê· ì´ë‚˜ ì¤‘ì•™ê°’ìœ¼ë¡œ ì±„ìš°ê¸° ì–´ë µê¸° ë•Œë¬¸ì— 'Missing'ì´ë¼ëŠ” í•˜ë‚˜ì˜ ìƒˆë¡œìš´ ê°’ìœ¼ë¡œ ì±„ì›Œì£¼ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. 

â—¾ ì´ë ‡ê²Œ í•˜ë©´ ëˆ„ë½ëœ ë°ì´í„°ë„ í•˜ë‚˜ì˜ ë²”ì£¼ë¡œ ê°„ì£¼í•´ì„œ ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. 

```python 
train[feature] = encoders[feature].fit_transform(train[feature])
test[feature] = encoders[feature].transform(test[feature])
```

â—¾ fit()ì€ ë²”ì£¼í˜• ê°’ë“¤ì„ ìˆ«ìë¡œ ë§¤í•‘í•˜ëŠ” ê¸°ì¤€(ì‚¬ì „)ì„ í•™ìŠµí•©ë‹ˆë‹¤.

â—¾ transform()ì€ ê·¸ ê¸°ì¤€ì— ë”°ë¼ ì‹¤ì œ ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤. 

â—¾ fit_transform()ì€ ì´ ë‘ê°€ì§€ ê¸°ëŠ¥ì„ í•œë²ˆì— ìˆ˜í–‰í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. 

âœ… train[feature]ì—ì„œëŠ” fit_transformì„ ì‚¬ìš©í•˜ê³  test[feature]ì—ì„œëŠ” transformì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 

    â–ª testì—ë§Œ ìˆëŠ” ê°’ì„ ìƒˆë¡­ê²Œ í•™ìŠµí•˜ë©´ trainê³¼ ë‹¤ë¥¸ ê¸°ì¤€ì´ ìƒê¸°ê¸° ë•Œë¬¸ì— ëª¨ë¸ ì…ì¥ì—ì„œ í—·ê°ˆë¦¬ê²Œ ë©ë‹ˆë‹¤.
    â–ª ë”°ë¼ì„œ testì—ì„œëŠ” í•­ìƒ trainì—ì„œ ë°°ìš´ ê¸°ì¤€ ê·¸ëŒ€ë¡œ transformë§Œ ìˆ˜í–‰í•˜ê²Œ ë©ë‹ˆë‹¤. 
    
âœ… ê³ ê°ìˆ˜ëŠ” ì „ì²´ í‰ê· ìœ¼ë¡œ, ì§ì›ìˆ˜ëŠ” êµ­ê°€ë³„ í‰ê· ìœ¼ë¡œ ë‚˜ëˆˆ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 

    â–ª ê³ ê°ìˆ˜(ë°±ë§Œëª…)ì€ ì „ ì„¸ê³„ì ìœ¼ë¡œ ê³ ë¥´ê²Œ ë¶„í¬ëœ ì§€í‘œì´ê¸° ë•Œë¬¸ì— ì „ì²´ í‰ê· ìœ¼ë¡œ ëŒ€ì²´í–ˆìŠµë‹ˆë‹¤. 
    â–ª ì§ì›ìˆ˜ëŠ” êµ­ê°€ë§ˆë‹¤ ê·œëª¨ë‚˜ ì±„ìš© ë°©ì‹ì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— êµ­ê°€ë³„ í‰ê· ìœ¼ë¡œ ëŒ€ì²´í–ˆìŠµë‹ˆë‹¤. 
    
```python
country_mean = train.groupby('êµ­ê°€')['ì§ì› ìˆ˜'].mean()
test['ì§ì› ìˆ˜'] = test.apply(
    lambda row: country_mean[row['êµ­ê°€']] if pd.isnull(row['ì§ì› ìˆ˜']) else row['ì§ì› ìˆ˜'], axis=1
)
```

â—¾ groupby('êµ­ê°€')['ì§ì›ìˆ˜'].mean() : êµ­ê°€ë³„ í‰ê·  ì§ì› ìˆ˜ ê³„ì‚° 

â—¾ apply() : test ë°ì´í„°ì˜ ê° í–‰ì„ ìˆœíšŒí•˜ë©´ì„œ 

â—¾ ì§ì› ìˆ˜ê°€ ê²°ì¸¡ì´ë©´ í•´ë‹¹ êµ­ê°€ì˜ í‰ê· ê°’ìœ¼ë¡œ ì±„ì›Œì£¼ê³  

â—¾ ê²°ì¸¡ì´ ì•„ë‹ˆë¼ë©´ ì›ë˜ ê°’ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•©ë‹ˆë‹¤. 

âœ… testì˜ ì§ì› ìˆ˜ ê²°ì¸¡ì¹˜ë¥¼ test ìì²´ì˜ í‰ê· ì´ ì•„ë‹Œ trainì—ì„œ ê³„ì‚°í•œ êµ­ê°€ë³„ í‰ê· ìœ¼ë¡œ ì±„ìš°ëŠ” ê²ƒì— ëŒ€í•œ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 

    â–ª ë°ì´í„° ëˆ„ìˆ˜(Data Leakage)ë¥¼ ë§‰ê¸° ìœ„í•´ì„œì…ë‹ˆë‹¤. 
    â–ª Data Leakangeë€? 
        â–« ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œ ë¯¸ë˜ ì •ë³´(testì˜ í†µê³„)ê°€ ì„ì—¬ ë“¤ì–´ê°€ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. 
        â–« ì´ëŸ¬ë©´ ëª¨ë¸ì´ ì‹¤ì œë¡œëŠ” ëª¨ë¥´ëŠ” ì •ë³´ë¥¼ ëª°ë˜ ë³¸ ì…ˆì´ ë¼ì„œ, í›ˆë ¨ ë• ì˜ë§ì·„ëŠ”ë° ì‹¤ì œë¡œëŠ” ëª»ë§ì¶”ëŠ” ëª¨ë¸ì´ ë  ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§‘ë‹ˆë‹¤. 
        â–« ë”°ë¼ì„œ ì‹¤ì œ ì˜ˆì¸¡ ìƒí™©ê³¼ ë™ì¼í•˜ê²Œ ëª¨ë¸ì„ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        â–« ë˜í•œ ì„±ëŠ¥ì´ ë¶€í’€ë ¤ì§€ì§€ ì•Šê³  ì •ì§í•œ í‰ê°€ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤. 
        
```python
train.columns = train.columns.str.replace(" ", "_")
test.columns = test.columns.str.replace(" ", "_")
```

â—¾ ì´í›„ ëª¨ë¸ì„ ì‹¤í–‰í•˜ëŠ” ê³¼ì •ì—ì„œ columnsì— ë„ì–´ì“°ê¸°ê°€ ìˆìœ¼ë©´ íŒŒì‹±í•  ë•Œ ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë¯¸ë¦¬ ê³µë°±ì„ ì œê±°í•©ë‹ˆë‹¤. 

# LGBMRegressorë¡œ K-Fold êµì°¨ê²€ì¦ ìˆ˜í–‰

---

```python
# âœ… Feature, Target ì •ì˜
X = train.drop(columns=['ì„±ê³µí™•ë¥ '])
y = train['ì„±ê³µí™•ë¥ ']

# âœ… ëª¨ë¸ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
models = []
cv_scores = []

# âœ… KFold ì„¤ì •
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# âœ… ëª¨ë¸ í•™ìŠµ
for fold, (train_idx, valid_idx) in enumerate(kf.split(X)):
    print(f"\nğŸ” Fold {fold+1}/5")

    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

    model = LGBMRegressor(
        n_estimators=1000,
        learning_rate=0.05,
        random_state=42
    )

    model.fit(
        X_train, y_train,
        eval_set=[(X_valid, y_valid)],
        callbacks=[
            early_stopping(stopping_rounds=50),
            log_evaluation(period=100)
        ]
    )

    models.append(model)
    score = model.best_score_['valid_0']['l2']
    cv_scores.append(score)
```

---
    
    ğŸ” Fold 1/5
    [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000270 seconds.
    You can set `force_row_wise=true` to remove the overhead.
    And if memory is not enough, you can set `force_col_wise=true`.
    [LightGBM] [Info] Total Bins 1165
    [LightGBM] [Info] Number of data points in the train set: 3500, number of used features: 12
    [LightGBM] [Info] Start training from score 0.534486
    Training until validation scores don't improve for 50 rounds
    Early stopping, best iteration is:
    [1]	valid_0's l2: 0.0579823
    
    ğŸ” Fold 2/5
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000510 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 1165
    [LightGBM] [Info] Number of data points in the train set: 3501, number of used features: 12
    [LightGBM] [Info] Start training from score 0.537104
    Training until validation scores don't improve for 50 rounds
    Early stopping, best iteration is:
    [11]	valid_0's l2: 0.0590495
    
    ğŸ” Fold 3/5
    [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000312 seconds.
    You can set `force_row_wise=true` to remove the overhead.
    And if memory is not enough, you can set `force_col_wise=true`.
    [LightGBM] [Info] Total Bins 1164
    [LightGBM] [Info] Number of data points in the train set: 3501, number of used features: 12
    [LightGBM] [Info] Start training from score 0.537332
    Training until validation scores don't improve for 50 rounds
    Early stopping, best iteration is:
    [11]	valid_0's l2: 0.0580187
    
    ğŸ” Fold 4/5
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000551 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 1164
    [LightGBM] [Info] Number of data points in the train set: 3501, number of used features: 12
    [LightGBM] [Info] Start training from score 0.538018
    Training until validation scores don't improve for 50 rounds
    Early stopping, best iteration is:
    [35]	valid_0's l2: 0.0575089
    
    ğŸ” Fold 5/5
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000552 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 1164
    [LightGBM] [Info] Number of data points in the train set: 3501, number of used features: 12
    [LightGBM] [Info] Start training from score 0.539760
    Training until validation scores don't improve for 50 rounds
    [100]	valid_0's l2: 0.0593835
    Early stopping, best iteration is:
    [56]	valid_0's l2: 0.0580773
    

```python
X = train.drop(columns=['ì„±ê³µí™•ë¥ '])
y = train['ì„±ê³µí™•ë¥ ']
```

â—¾ X : ëª¨ë¸ì´ í•™ìŠµí•  ì…ë ¥ê°’(í”¼ì²˜) 'ì„±ê³µí™•ë¥ 'ì€ ì˜ˆì¸¡í•˜ë ¤ëŠ” ê°’ì´ê¸° ë•Œë¬¸ì— dropí•©ë‹ˆë‹¤.

â—¾ y : ìš°ë¦¬ê°€ ì˜ˆì¸¡í•˜ê³  ì‹¶ì€ íƒ€ê²Ÿê°’ì´ 'ì„±ê³µí™•ë¥ 'ì´ê¸° ë•Œë¬¸ì— ì´ë ‡ê²Œ ì„¤ì •í•©ë‹ˆë‹¤. 

```python
models = []
cv_scores = []
```

â—¾ models : ê° foldì—ì„œ í•™ìŠµëœ ëª¨ë¸ì„ ì €ì¥í•´ë‘ëŠ” ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ë‚˜ì¤‘ì— ì•™ìƒë¸” í•  ë•Œ ì“¸ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. 

    âœ… ì•™ìƒë¸”?
    â–ª ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²°í•©í•´ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì–»ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. 
    â–ª ê° foldì—ì„œ í•™ìŠµëœ ëª¨ë¸ì€ ì¡°ê¸ˆì”© ë‹¤ë¥¸ ë°ì´í„°ë¥¼ ë³´ê³  í•™ìŠµí•˜ì˜€ìŠµë‹ˆë‹¤. 
        â–« ë”°ë¼ì„œ ì–´ë–¤ ëª¨ë¸ì€ íŠ¹ì • íŒ¨í„´ì„ ì˜ ë³´ê³  ì–´ë–¤ ëª¨ë¸ì€ ë‹¤ë¥¸ ë¶€ë¶„ì—ì„œ ì˜ ì˜ˆì¸¡í•˜ëŠ” ê²½í–¥ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    â–ª ê·¸ë˜ì„œ ê° ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‰ê· í•˜ê±°ë‚˜ ê°€ì¤‘í‰ê· , ë˜ëŠ” ë‹¤ìˆ˜ê²° ë“±ìœ¼ë¡œ ì´ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
        â–« ê°œë³„ ëª¨ë¸ì˜ ì•½ì €ì„ ë³´ì™„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
        â–« ê²°ê³¼ì ìœ¼ë¡œ ë” ì•ˆì •ì ì¸ ì˜ˆì¸¡ì´ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤. 

â—¾ cv_scroes : ê° foldì—ì„œ ì–»ì€ ê²€ì¦ ì ìˆ˜ (L2 loss)ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. 

```python 
kf = KFold(n_splits=5, shuffle=True, random_state=42)
```

â—¾ n_splits = 5 : ë°ì´í„°ë¥¼ 5ê°œë¡œ ë‚˜ëˆ ì„œ êµì°¨ê²€ì¦ì„ í•˜ê² ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. 

â—¾ shuffle = True : ë°ì´í„°ë¥¼ ì„ì–´ì„œ ì¢€ ë” ë¬´ì‘ìœ„ì„± ìˆê²Œ ë‚˜ëˆ„ê¸° ìœ„í•´ ì„¤ì •í•©ë‹ˆë‹¤. 

â—¾ random_state = 42 : ëœë¤í•œ ë¶„í• ì„ í•­ìƒ ë™ì¼í•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•œ ê³ ì • ì‹œë“œì…ë‹ˆë‹¤. 

```python 
for fold, (train_idx, valid_idx) in enumerate(kf.split(X)):
```

â—¾ kf.split(X)ì€ í•™ìŠµìš© indexì™€ ê²€ì¦ìš© indexë¥¼ 5ë²ˆ ë§Œë“¤ì–´ë‹¬ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. 

â—¾ foldëŠ” í˜„ì¬ ëª‡ ë²ˆì§¸ foldì¸ì§€ ì•Œë ¤ì£¼ëŠ” ë²ˆí˜¸ì˜ ì‹œì‘ì…ë‹ˆë‹¤. (0ë¶€í„° ì‹œì‘)

```python
X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]
```

â—¾ foldë§ˆë‹¤ ë¶„ë¦¬ëœ í•™ìŠµ / ê²€ì¦ ë°ì´í„°ë¥¼ ì‹¤ì œ ë°ì´í„°ë¡œ ë‚˜ëˆ ì¤ë‹ˆë‹¤. 

```python 
model = LGBMRegressor(
    n_estimators=1000,
    learning_rate=0.05,
    random_state=42
)
```

â—¾ n_estimators : ìµœëŒ€ 1000ê°œì˜ íŠ¸ë¦¬ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ë„ˆë¬´ ë§ìœ¼ë©´ ì˜¤ë²„í”¼íŒ…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.)

â—¾ learning_rate : í•™ìŠµ ì†ë„ë¥¼ ê²°ì •í•˜ëŠ” íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤. ì‘ì„ìˆ˜ë¡ ì²œì²œíˆ, í•˜ì§€ë§Œ ë” ì •ë°€í•˜ê²Œ í•™ìŠµí•©ë‹ˆë‹¤. 

â—¾ random_state : ê²°ê³¼ ì¬í˜„ ê°€ëŠ¥í•˜ë„ë¡ ê³ ì •ê°’ì„ ì„¤ì •í•©ë‹ˆë‹¤ .

```python 
model.fit(
    X_train, y_train,
    eval_set=[(X_valid, y_valid)],
    callbacks=[
        early_stopping(stopping_rounds=50),
        log_evaluation(period=100)
    ]
)
```

â—¾ eval_set : ê²€ì¦ ë°ì´í„°ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ëª¨ë¸ ì„±ëŠ¥ì„ ì²´í¬í•©ë‹ˆë‹¤. 

â—¾ early_stopping : ê²€ì¦ ì„±ëŠ¥ì´ 50ë²ˆ ì—°ì† ì¢‹ì•„ì§€ì§€ ì•Šìœ¼ë©´ í•™ìŠµì„ ì¼ì° ì¢…ë£Œí•©ë‹ˆë‹¤. 

â—¾ log_evaluation : 100ë²ˆë§ˆë‹¤ í•™ìŠµ ë¡œê·¸ë¥¼ ì¶œë ¥í•´ì¤˜ì„œ ê²½ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

```python
models.append(model)
score = model.best_score_['valid_0']['l2']
cv_scores.append(score)
```

â—¾ í•™ìŠµëœ ë°ì´í„°ë¥¼ modelsì— append í•©ë‹ˆë‹¤.

â—¾ ê²€ì¦ ë°ì´í„°ì—ì„œì˜ L2 Loss(í‰ê· ì œê³±ì˜¤ì°¨)ë¥¼ ì €ì •í•©ë‹ˆë‹¤. 

âœ… êµì°¨ê²€ì¦ì„ í•˜ëŠ” ì´ìœ  
    
    â–ª í•˜ë‚˜ì˜ ê³ ì •ëœ ê²€ì¦ì…‹ìœ¼ë¡œë§Œ í‰ê°€í•˜ë©´ ìš´ì´ ì¢‹ê±°ë‚˜ ë‚˜ìœ ê²½ìš°ë„ í¬í•¨ë¼ì„œ ì„±ëŠ¥ì´ í”ë“¤ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
    â–ª ê·¸ë˜ì„œ ì—¬ëŸ¬ ë²ˆ ë‚˜ëˆ ì„œ í•™ìŠµí•´ë³´ê³ , í‰ê· ì ì¸ ì„±ëŠ¥ì„ ë³´ëŠ” ê²ƒì´ ë” ì•ˆì •ì ì…ë‹ˆë‹¤. 

ğŸ” ê²°ê³¼ ë¶„ì„

â—¾ Fold 1/5 
    
    â–ª best iteration = 1
    â–ª l2 = 0.0579823 
        â†’ ëª¨ë¸ì´ 1ë²ˆì§¸ íŠ¸ë¦¬ì—ì„œ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì•˜ìŠµë‹ˆë‹¤.
        â†’ ì¦‰, ì´ˆë°˜ì— ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ê³  ì´í›„ë¡œëŠ” ê°œì„ ì´ ì•ˆëœë‹¤ëŠ” ì˜ë¯¸ì´ë¯€ë¡œ ê³¼ì í•© ë  ê°€ëŠ¥ì„±ì´ ì¡´ì¬í•©ë‹ˆë‹¤. 
        
â—¾ Fold 2/5 
    
    â–ª best iteration = 11
    â–ª l2 = 0.0590495 
        â†’ 11ë²ˆì§¸ íŠ¸ë¦¬ì—ì„œ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ê³ , ê·¸ ì´í›„ë¡œëŠ” ë” ë‚˜ì•„ì§€ì§€ ì•Šì•„ì„œ early stopping
        â†’ ì—¬ì „íˆ ì¢‹ì€ ì„±ëŠ¥ì…ë‹ˆë‹¤. 

â—¾ Fold 3/5 
    
    â–ª best iteration = 11
    â–ª l2 = 0.0580187
        â†’ Fold 2ì™€ ê±°ì˜ ìœ ì‚¬í•©ë‹ˆë‹¤. ëª¨ë¸ì´ ë¹„êµì  ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ëŠ” ëª¨ìŠµì…ë‹ˆë‹¤. 

â—¾ Fold 4/5 
    
    â–ª best iteration = 35
    â–ª l2 = 0.0575089
        â†’ ê°€ì¥ ë‚®ì€ ì˜¤ì°¨ì…ë‹ˆë‹¤. ì„±ëŠ¥ë„ ê°€ì¥ ì¢‹ê³ , 35ë²ˆê¹Œì§€ í•™ìŠµí•œ ê²ƒìœ¼ë¡œ ë³´ì•„ ì¡°ê¸ˆ ë” ë³µì¡í•œ Fold ë°ì´í„° ì¸ ê²ƒìœ¼ë¡œ ì˜ˆì¸¡ë©ë‹ˆë‹¤. 

â—¾ Fold 5/5 
    
    â–ª best iteration = 56
    â–ª l2 = 0.0580773
        â†’ ê½¤ ë§ì€ íŠ¸ë¦¬ ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì˜€ê³ , ì„±ëŠ¥ì€ í‰ê·  ìˆ˜ì¤€ì…ë‹ˆë‹¤.

# LGBM K-Fold Model Prediction

---

```python
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ featureë§Œ ì¶”ì¶œ
X_test = test  # ì—¬ê¸°ì„œë§Œ feature ì¶”ì¶œìš©ìœ¼ë¡œ ì‚¬ìš©

# ëª¨ë“  ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ìš©
predictions = []

for model in models:
    preds = model.predict(X_test)
    predictions.append(preds)

# ëª¨ë¸ë“¤ì˜ í‰ê·  ì˜ˆì¸¡
final_prediction = np.mean(predictions, axis=0)
```

---

```python
X_test = test
```

â—¾ test ë°ì´í„°ì—ëŠ” 'ì„±ê³µí™•ë¥ 'ì´ ì—†ê¸° ë•Œë¬¸ì— ê·¸ëŒ€ë¡œ featureë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. 

```python
prediction = []
```

â—¾ ëª¨ë¸ë³„ ì˜ˆì¸¡ê°’ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤. 

```python
for model in models : 
```

â—¾ êµì°¨ê²€ì¦ì—ì„œ ì €ì¥í•œ 5ê°œì˜ ëª¨ë¸ì„ í•˜ë‚˜ì”© êº¼ë‚´ì„œ ë°˜ë³µë¬¸ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. 

```python
model.prediction(X_test)
```

â—¾ ê° ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. 

```python
np.mean(predictions, axis = 0)
```

â—¾ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ì„ í‰ê·  ë‚´ì–´ ìµœì¢… ì˜ˆì¸¡ê°’ì„ ìƒì„±í•©ë‹ˆë‹¤. 

# Submission

---

```python
# ì œì¶œ ì–‘ì‹ì— ë§ê²Œ ID + ì˜ˆì¸¡ê°’ ë¶™ì´ê¸°
sample_submission['ì„±ê³µí™•ë¥ '] = final_prediction
sample_submission.to_csv('./baseline_submission.csv', index = False, encoding = 'utf-8-sig')
```

---

```python
sample_submission['ì„±ê³µí™•ë¥ '] = final_prediction
```
â—¾ ì œì¶œ ì–‘ì‹(sample_submission)ì— ë§Œë“  ì˜ˆì¸¡ ê²°ê³¼(final_prediction)ì„ ë„£ì–´ì¤ë‹ˆë‹¤.

â—¾ ì—¬ê¸°ì„œ 'ì„±ê³µí™•ë¥ 'ì€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ, ëŒ€íšŒì—ì„œ ìš”êµ¬í•œ ì´ë¦„ê³¼ ì¼ì¹˜í•©ë‹ˆë‹¤. 


```python
sample_submission.to_csv('./baseline_submission.csv', index = False, encoding = 'utf-8-sig')
```

â—¾ index = False ëŠ” í–‰ ë²ˆí˜¸(index)ë¥¼ ì €ì¥í•˜ì§€ ì•Šë„ë¡ ì„¤ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. 

â—¾ encoding = 'utf-8-sig' ëŠ” í•œê¸€ ê¹¨ì§ ë°©ì§€ìš© ì¸ì½”ë”©ì…ë‹ˆë‹¤.

ğŸ“Š Public Leaderboard ì„±ëŠ¥

ğŸ“Œ ì œì¶œ ì ìˆ˜ (Weighted MAE): **0.22411**

ğŸ… ë“±ìˆ˜: **62ë“±**
