---
title: "[Dacon ê¸°ì—… ì„±ê³µ ì˜ˆì¸¡] Catboostë¥¼ í†µí•œ Dacon ê¸°ì—… ì„±ê³µ ì˜ˆì¸¡"

categories: Competitions&Challenges

tags: [python, Catboost, KFold, Dacon, Hyper_Parameters, Feature_Engineering, Tuning, Clipping]
---

# Dacon ê¸°ì—… ì„±ê³µ ì˜ˆì¸¡

[ë§í¬](https://dacon.io/competitions/official/236475/overview/description)

âœ… ë‹¤ì–‘í•œ ê¸°ì—… ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ AI ì•Œê³ ë¦¬ì¦˜ì„ ê°œë°œí•˜ì—¬, ê¸°ì—…ì˜ ì„±ê³µ ê°€ëŠ¥ì„±ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. 

âœ… ì—¬ëŸ¬ ëª¨ë¸ì„ ê²½í—˜í•˜ê³  ë‹¤ì–‘í•œ ê¸°ë²•, ë‹¤ì–‘í•œ ë°©ë²•ì„ ê²½í—˜í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ ì„í•˜ì˜€ìŠµë‹ˆë‹¤. 

# Import


```python
import pandas as pd
import numpy as np
import optuna
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import KFold
from catboost import CatBoostRegressor, Pool
```

# Data Load


```python
train = pd.read_csv('Dacon_data/train.csv')
test = pd.read_csv('Dacon_data/test.csv')
sample_submission = pd.read_csv('Dacon_data/sample_submission.csv')
```

# Data Preprocessing


```python
# ê¸°ì¤€ ì—°ë„ ì„¤ì •
CURRENT_YEAR = 2025

# ê¸°ì—… ë‚˜ì´ ìƒì„±
train['ê¸°ì—…ë‚˜ì´'] = CURRENT_YEAR - train['ì„¤ë¦½ì—°ë„']
test['ê¸°ì—…ë‚˜ì´'] = CURRENT_YEAR - test['ì„¤ë¦½ì—°ë„']

# ê¸°ì¡´ ì»¬ëŸ¼ ì œê±°
train = train.drop(columns=['ì„¤ë¦½ì—°ë„'])
test = test.drop(columns=['ì„¤ë¦½ì—°ë„'])

train = train.drop(columns=['ID'], axis = 1)
test = test.drop(columns=['ID'], axis = 1)

# ìˆ˜ì¹˜í˜• ì²˜ë¦¬
numeric_features = ['ê¸°ì—…ë‚˜ì´', 'ì§ì› ìˆ˜', 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)', 'ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'ì—°ë§¤ì¶œ(ì–µì›)', 'SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)']

# ì´ì§„í˜• ì²˜ë¦¬
bool_features = ['ì¸ìˆ˜ì—¬ë¶€', 'ìƒì¥ì—¬ë¶€']
bool_map = {'Yes': 1, 'No': 0}
for col in bool_features :
    train[col] = train[col].map(bool_map)
    test[col] = test[col].map(bool_map)

# ë²”ì£¼í˜• ì²˜ë¦¬
category_features = ['êµ­ê°€', 'ë¶„ì•¼', 'íˆ¬ìë‹¨ê³„', 'ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)']
encoders = {}

for feature in category_features:
    encoders[feature] = LabelEncoder()
    train[feature] = train[feature].fillna('Missing')
    test[feature] = test[feature].fillna('Missing')
    train[feature] = encoders[feature].fit_transform(train[feature])
    test[feature] = encoders[feature].transform(test[feature])
    
# ê³ ê°ìˆ˜(ë°±ë§Œëª…): ì „ì²´ í‰ê· ìœ¼ë¡œ ì±„ìš°ê¸°
customer_mean = train['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'].mean()
train['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'] = train['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'].fillna(customer_mean)
test['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'] = test['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'].fillna(customer_mean)

# ì§ì› ìˆ˜: êµ­ê°€ë³„ í‰ê· ìœ¼ë¡œ ì±„ìš°ê¸°
train['ì§ì› ìˆ˜'] = train.groupby('êµ­ê°€')['ì§ì› ìˆ˜'].transform(lambda x: x.fillna(x.mean()))

# test ë°ì´í„°ëŠ” êµ­ê°€ë³„ í‰ê· ì„ trainì—ì„œ ê°€ì ¸ì™€ì„œ ì±„ìš°ê¸°
country_mean = train.groupby('êµ­ê°€')['ì§ì› ìˆ˜'].mean()
test['ì§ì› ìˆ˜'] = test.apply(
    lambda row: country_mean[row['êµ­ê°€']] if pd.isnull(row['ì§ì› ìˆ˜']) else row['ì§ì› ìˆ˜'], axis=1
)

## Feature Engineering 
train['íˆ¬ì ëŒ€ë¹„ ë§¤ì¶œ'] = train['ì—°ë§¤ì¶œ(ì–µì›)'] / (train['ì´ íˆ¬ìê¸ˆ(ì–µì›)'] + 1)
test['íˆ¬ì ëŒ€ë¹„ ë§¤ì¶œ'] = test['ì—°ë§¤ì¶œ(ì–µì›)'] / (test['ì´ íˆ¬ìê¸ˆ(ì–µì›)'] + 1)

# 1ì¸ë‹¹ ë§¤ì¶œ
train['1ì¸ë‹¹ ë§¤ì¶œ'] = train['ì—°ë§¤ì¶œ(ì–µì›)'] / (train['ì§ì› ìˆ˜'] + 1)
test['1ì¸ë‹¹ ë§¤ì¶œ'] = test['ì—°ë§¤ì¶œ(ì–µì›)'] / (test['ì§ì› ìˆ˜'] + 1)

# 1ì¸ë‹¹ íˆ¬ìê¸ˆ
train['1ì¸ë‹¹ íˆ¬ìê¸ˆ'] = train['ì´ íˆ¬ìê¸ˆ(ì–µì›)'] / (train['ì§ì› ìˆ˜'] + 1)
test['1ì¸ë‹¹ íˆ¬ìê¸ˆ'] = test['ì´ íˆ¬ìê¸ˆ(ì–µì›)'] / (test['ì§ì› ìˆ˜'] + 1)

# 1ì¸ë‹¹ ê³ ê° ìˆ˜
train['1ì¸ë‹¹ ê³ ê°'] = train['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'] / (train['ì§ì› ìˆ˜'] + 1)
test['1ì¸ë‹¹ ê³ ê°'] = test['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'] / (test['ì§ì› ìˆ˜'] + 1)

# ê³ ê°ë‹¹ ì—°ë§¤ì¶œ
train['ê³ ê°ë‹¹ ì—°ë§¤ì¶œ'] = train['ì—°ë§¤ì¶œ(ì–µì›)'] / (train['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'] + 1)
test['ê³ ê°ë‹¹ ì—°ë§¤ì¶œ'] = test['ì—°ë§¤ì¶œ(ì–µì›)'] / (test['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'] + 1)

# SNS íŒ”ë¡œì›Œ ëŒ€ë¹„ ì—°ë§¤ì¶œ
train['SNS íŒ”ë¡œì›Œ ëŒ€ë¹„ ë§¤ì¶œ'] = train['ì—°ë§¤ì¶œ(ì–µì›)'] / (train['SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)'] + 1)
test['SNS íŒ”ë¡œì›Œ ëŒ€ë¹„ ë§¤ì¶œ'] = test['ì—°ë§¤ì¶œ(ì–µì›)'] / (test['SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)'] + 1)

# ê¸°ì—…ê°€ì¹˜ ëŒ€ë¹„ ì—°ë§¤ì¶œ
train['ê°€ì¹˜ ëŒ€ë¹„ ë§¤ì¶œ'] = train['ì—°ë§¤ì¶œ(ì–µì›)'] / (train['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'] + 1)
test['ê°€ì¹˜ ëŒ€ë¹„ ë§¤ì¶œ'] = test['ì—°ë§¤ì¶œ(ì–µì›)'] / (test['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'] + 1)

# ë¡œê·¸ ë³€í™˜: ì™œê³¡ì´ ì‹¬í•œ ì»¬ëŸ¼
for col in ['ì—°ë§¤ì¶œ(ì–µì›)', 'ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'ì§ì› ìˆ˜', 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)', 'SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)']:
    train[f'log_{col}'] = np.log1p(train[col])
    test[f'log_{col}'] = np.log1p(test[col])
    
train = train.drop(columns = ['ì—°ë§¤ì¶œ(ì–µì›)', 'ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'ì§ì› ìˆ˜', 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)', 'SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)'])
test = test.drop(columns = ['ì—°ë§¤ì¶œ(ì–µì›)', 'ì´ íˆ¬ìê¸ˆ(ì–µì›)', 'ì§ì› ìˆ˜', 'ê³ ê°ìˆ˜(ë°±ë§Œëª…)', 'SNS íŒ”ë¡œì›Œ ìˆ˜(ë°±ë§Œëª…)'])
    
# ì»¬ëŸ¼ëª…ì˜ ë„ì–´ì“°ê¸°ë¥¼ ì–¸ë”ë°”ë¡œ ëŒ€ì²´
train.columns = train.columns.str.replace(" ", "_")
test.columns = test.columns.str.replace(" ", "_")

train = train.drop(columns = ["ì¸ìˆ˜ì—¬ë¶€", "ìƒì¥ì—¬ë¶€"])
test = test.drop(columns = ["ì¸ìˆ˜ì—¬ë¶€", "ìƒì¥ì—¬ë¶€"])
```

[Feature Engineering](https://cjh980225.github.io/model_training&tuning/ML_2)

# Catboost Hyper Parameter

```python
# âœ… Feature, Target ì •ì˜
X = train.drop(columns=['ì„±ê³µí™•ë¥ '])
y = train['ì„±ê³µí™•ë¥ ']

# âœ… Optuna objective í•¨ìˆ˜ ì •ì˜
def objective(trial):
    params = {
        'iterations': 3000,
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'depth': trial.suggest_int('depth', 4, 10),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),
        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),
        'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0, log=True),
        'border_count': trial.suggest_int('border_count', 64, 254),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 20),
        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),
        'loss_function': 'MAE',
        'early_stopping_rounds': 100,
        'verbose': 0,
        'random_state': 42
    }

    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    mae_list = []

    for train_idx, valid_idx in kf.split(X):
        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

        train_pool = Pool(X_train, y_train)
        valid_pool = Pool(X_valid, y_valid)

        model = CatBoostRegressor(**params)
        model.fit(train_pool, eval_set=valid_pool, use_best_model=True)

        preds = model.predict(valid_pool)
        preds = np.clip(preds, 0, 1)  # â† ì´ ì¤„ ì¶”ê°€
        mae = mean_absolute_error(y_valid, preds)
        mae_list.append(mae)

    return np.mean(mae_list)

# âœ… íŠœë‹ ì‹œì‘
sampler = optuna.samplers.TPESampler(seed=42)
study = optuna.create_study(direction='minimize', sampler=sampler)
study.optimize(objective, n_trials=100)

# âœ… ê²°ê³¼ ì¶œë ¥
print("Best trial:")
print(study.best_trial.params)
```

    (ì¤‘ëµ)
    
    Best trial:
    {'learning_rate': 0.02528730846845928, 'depth': 10, 'l2_leaf_reg': 0.1589791488312824, 'bagging_temperature': 0.054469271620564606, 'random_strength': 0.002793402531131714, 'border_count': 242, 'min_data_in_leaf': 1, 'grow_policy': 'Depthwise'}
    

# Catboostë¡œ K-Fold êµì°¨ê²€ì¦ ìˆ˜í–‰


```python
# âœ… Feature, Target ì •ì˜
X = train.drop(columns=['ì„±ê³µí™•ë¥ '])
y = train['ì„±ê³µí™•ë¥ ']

# âœ… best_params ê°€ì ¸ì˜¤ê¸° (ë³µì‚¬ í›„ ìˆ˜ì • ê¶Œì¥)
best_params = study.best_trial.params.copy()

# âœ… ì¶”ê°€ë¡œ í•„ìš”í•œ ê³ ì • íŒŒë¼ë¯¸í„° ë„£ê¸°
best_params.update({
    'iterations': 3000,
    'loss_function': 'MAE',
    'random_state': 42
})

# âœ… ëª¨ë¸ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
models = []

# âœ… 10-Fold êµì°¨ê²€ì¦
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# âœ… ì˜ˆì¸¡ê°’ ì €ì¥ìš©
val_scores = []

for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
    print(f"ğŸ” Fold {fold+1}/5")

    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # âœ… Pool ê°ì²´ ì •ì˜
    train_pool = Pool(X_train, y_train)
    val_pool = Pool(X_val, y_val)

    # âœ… ëª¨ë¸ ì •ì˜
    model = CatBoostRegressor(**best_params)
    model.fit(train_pool, eval_set=val_pool, use_best_model=True)

    # âœ… ëª¨ë¸ ì €ì¥
    models.append(model)

    # âœ… ê²€ì¦ ì ìˆ˜ ì €ì¥ (MAE ê¸°ì¤€)
    val_pred = model.predict(val_pool)
    val_pred = np.clip(val_pred, 0, 1)  # âœ… ì—¬ê¸°ë„ í•„ìš”
    score = mean_absolute_error(y_val, val_pred)
    print(f"âœ… Fold {fold+1} MAE: {score:.5f}")
    val_scores.append(score)

# âœ… ì „ì²´ í‰ê·  ì ìˆ˜ ì¶œë ¥
print(f"\nğŸ“Š í‰ê·  MAE (íŠœë‹ëœ CatBoost): {np.mean(val_scores):.5f}")

```

    (ì¤‘ëµ)

    Shrink model to first 2353 iterations.
    âœ… Fold 5 MAE: 0.19837
    
    ğŸ“Š í‰ê·  MAE (íŠœë‹ëœ CatBoost): 0.19780
    

# Catboost K-Fold Model Prediction


```python
X_test = test

# ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ìš©
predictions = []

for model in models:
    preds = model.predict(X_test)
    predictions.append(preds)

# ëª¨ë¸ì˜ í‰ê·  ì˜ˆì¸¡
final_prediction = np.mean(predictions, axis=0)

# ìµœì¢… ì˜ˆì¸¡ê°’ í´ë¦¬í•‘
final_prediction = np.clip(final_prediction, 0, 1)
```

[Hyper Parameters](https://cjh980225.github.io/model_training&tuning/ML_3)


# Submission

```python
# ì œì¶œ ì–‘ì‹ì— ë§ê²Œ ID + ì˜ˆì¸¡ê°’ ë¶™ì´ê¸°
sample_submission['ì„±ê³µí™•ë¥ '] = final_prediction
sample_submission.to_csv('./baseline_submission.csv', index = False, encoding = 'utf-8-sig')
```

# Result 

ğŸ“Œ ì œì¶œ ì ìˆ˜ : 0.21235

ğŸ… ë“±ìˆ˜ (2025.04.26 ê¸°ì¤€): 12ë“±

# Library version 

```python
import pandas as pd
import numpy as np
import optuna
import sklearn
import catboost
import sys


print("Python:", sys.version)
print("pandas:", pd.__version__)
print("numpy:", np.__version__)
print("optuna:", optuna.__version__)
print("scikit-learn:", sklearn.__version__)
print("catboost:", catboost.__version__)
```

    Python: 3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]
    pandas: 2.2.3
    numpy: 1.26.4
    optuna: 4.2.1
    scikit-learn: 1.6.1
    catboost: 1.2.7