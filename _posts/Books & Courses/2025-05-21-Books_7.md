---

title: "[ì´ê²ƒì´ ë°ì´í„° ë¶„ì„ì´ë‹¤] íƒ€ì´íƒ€ë‹‰ì˜ ìƒì¡´ì ê°€ë ¤ë‚´ê¸°"

categories: Books&Courses

tags: [Book, ì´ê²ƒì´_ë°ì´í„°_ë¶„ì„ì´ë‹¤, pandas, visualization, analysis]

---

# íƒ€ì´íƒ€ë‹‰ì˜ ìƒì¡´ì ê°€ë ¤ë‚´ê¸° 

## íƒìƒ‰ : íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ì‚´í´ë³´ê¸° 

|ë³€ìˆ˜|ë‚´ìš©|
|-|-|
|pclass|Passenger Class, ìŠ¹ê° ë“±ê¸‰|
|survived|ìƒì¡´ ì—¬ë¶€(ìƒì¡´ì€ 1, ì•„ë‹Œ ê²½ìš°ëŠ” 0)|
|name|ìŠ¹ê° ì´ë¦„|
|sex|ìŠ¹ê° ì„±ë³„|
|age|ìŠ¹ê° ë‚˜ì´|
|sibsp|ë™ìŠ¹í•œ í˜•ì œ ë˜ëŠ” ë°°ìš°ì ìˆ˜|
|parch|ë™ìŠ¹í•œ ë¶€ëª¨ ë˜ëŠ” ìë…€ ìˆ˜|
|ticket|í‹°ì¼“ ë²ˆí˜¸|
|fare|ìŠ¹ê° ì§€ë¶ˆ ìš”ê¸ˆ|
|cabin|ì„ ì‹¤ ì´ë¦„|
|embarked|ìŠ¹ì„ í•­(C = ì‰˜ë¶€ë¥´í¬, Q = í€¸ì¦ˆíƒ€ìš´, S = ì‚¬ìš°ìŠ¤ í–„íŠ¼)|
|body|ì‚¬ë§ì í™•ì¸ ë²ˆí˜¸|
|home.dest|ê³ í–¥/ëª©ì ì§€|

ğŸ” íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ì˜ ê¸°ë³¸ ì •ë³´ êµ¬í•˜ê¸° 


```python
# -*- coding : utf-8 -*- 
%matplotlib inline 

import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns 

df_train = pd.read_csv("./data/titanic_train.csv")
df_test = pd.read_csv("./data/titanic_test.csv")
df_train.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pclass</th>
      <th>survived</th>
      <th>name</th>
      <th>sex</th>
      <th>age</th>
      <th>sibsp</th>
      <th>parch</th>
      <th>ticket</th>
      <th>fare</th>
      <th>cabin</th>
      <th>embarked</th>
      <th>body</th>
      <th>home.dest</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2</td>
      <td>1</td>
      <td>Mellinger, Miss. Madeleine Violet</td>
      <td>female</td>
      <td>13.0</td>
      <td>0</td>
      <td>1</td>
      <td>250644</td>
      <td>19.5000</td>
      <td>NaN</td>
      <td>S</td>
      <td>NaN</td>
      <td>England / Bennington, VT</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>Wells, Miss. Joan</td>
      <td>female</td>
      <td>4.0</td>
      <td>1</td>
      <td>1</td>
      <td>29103</td>
      <td>23.0000</td>
      <td>NaN</td>
      <td>S</td>
      <td>NaN</td>
      <td>Cornwall / Akron, OH</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>1</td>
      <td>Duran y More, Miss. Florentina</td>
      <td>female</td>
      <td>30.0</td>
      <td>1</td>
      <td>0</td>
      <td>SC/PARIS 2148</td>
      <td>13.8583</td>
      <td>NaN</td>
      <td>C</td>
      <td>NaN</td>
      <td>Barcelona, Spain / Havana, Cuba</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>0</td>
      <td>Scanlan, Mr. James</td>
      <td>male</td>
      <td>NaN</td>
      <td>0</td>
      <td>0</td>
      <td>36209</td>
      <td>7.7250</td>
      <td>NaN</td>
      <td>Q</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>1</td>
      <td>Bradley, Miss. Bridget Delia</td>
      <td>female</td>
      <td>22.0</td>
      <td>0</td>
      <td>0</td>
      <td>334914</td>
      <td>7.7250</td>
      <td>NaN</td>
      <td>Q</td>
      <td>NaN</td>
      <td>Kingwilliamstown, Co Cork, Ireland Glens Falls...</td>
    </tr>
  </tbody>
</table>
</div>




```python
print(df_train.info())
print("-----------------------------------------------------")
print(df_test.info())
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 916 entries, 0 to 915
    Data columns (total 13 columns):
     #   Column     Non-Null Count  Dtype  
    ---  ------     --------------  -----  
     0   pclass     916 non-null    int64  
     1   survived   916 non-null    int64  
     2   name       916 non-null    object 
     3   sex        916 non-null    object 
     4   age        741 non-null    float64
     5   sibsp      916 non-null    int64  
     6   parch      916 non-null    int64  
     7   ticket     916 non-null    object 
     8   fare       916 non-null    float64
     9   cabin      214 non-null    object 
     10  embarked   914 non-null    object 
     11  body       85 non-null     float64
     12  home.dest  527 non-null    object 
    dtypes: float64(3), int64(4), object(6)
    memory usage: 93.2+ KB
    None
    -----------------------------------------------------
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 393 entries, 0 to 392
    Data columns (total 13 columns):
     #   Column     Non-Null Count  Dtype  
    ---  ------     --------------  -----  
     0   pclass     393 non-null    int64  
     1   survived   393 non-null    int64  
     2   name       393 non-null    object 
     3   sex        393 non-null    object 
     4   age        305 non-null    float64
     5   sibsp      393 non-null    int64  
     6   parch      393 non-null    int64  
     7   ticket     393 non-null    object 
     8   fare       393 non-null    float64
     9   cabin      81 non-null     object 
     10  embarked   393 non-null    object 
     11  body       36 non-null     float64
     12  home.dest  218 non-null    object 
    dtypes: float64(3), int64(4), object(6)
    memory usage: 40.0+ KB
    None
    

ğŸ” ë¶ˆí•„ìš”í•œ í”¼ì²˜ ì œê±°í•˜ê¸°


```python
# ë°ì´í„°ì…‹ì—ì„œ name, ticket, body, cabin, home.dest í”¼ì²˜ë¥¼ ì œê±°í•©ë‹ˆë‹¤. 
df_train = df_train.drop(['name', 'ticket', 'body', 'cabin', 'home.dest'], axis = 1)
df_test = df_test.drop(['name', 'ticket', 'body', 'cabin', 'home.dest'], axis = 1)
```

ğŸ” íƒìƒ‰ì  ë°ì´í„° ë¶„ì„í•˜ê¸° 


```python
print(df_train['survived'].value_counts())
df_train['survived'].value_counts().plot.bar()
```

    survived
    0    563
    1    353
    Name: count, dtype: int64
    




    <Axes: xlabel='survived'>




    
![png](/assets/images/Book/7/output_6_2.png)
    



```python
# survived í”¼ì²˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹ì„ ë‚˜ëˆ„ì–´ ê·¸ë£¹ë³„ pclass í”¼ì²˜ì˜ ë¶„í¬ë¥¼ ì‚´í´ë´…ë‹ˆë‹¤. 
print(df_train['pclass'].value_counts())
ax = sns.countplot(x = 'pclass', hue = 'survived', data = df_train)
```

    pclass
    3    498
    1    230
    2    188
    Name: count, dtype: int64
    


    
![png](/assets/images/Book/7/output_7_1.png)
    


ğŸ” Shapiro-wilk ê²€ì •

â–ª Shapiro-wilk ê²€ì •ì´ë€ ì£¼ì–´ì§„ ë°ì´í„°ê°€ ì–¼ë§ˆë‚˜ ì •ê·œì„±ì„ ë”°ë¥´ëŠ”ì§€, ì¦‰ ì–¼ë§ˆë‚˜ ì •ê·œë¶„í¬ì— ê°€ê¹Œìš´ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ê²€ì •ì…ë‹ˆë‹¤. 

ğŸ” ë³€ìˆ˜ íƒìƒ‰ì‘ì—… ìë™í™”í•˜ê¸° 


```python
from scipy import stats 

# ë‘ ì§‘ë‹¨ì˜ í”¼ì²˜ë¥¼ ë¹„êµí•´ì£¼ë©° íƒìƒ‰ì‘ì—…ì„ ìë™í™”í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. 
def valid_features(df, col_name, distribution_check = True) : 

    # ë‘ ì§‘ë‹¨ (survivied = 1, survivied = 0)ì˜ ë¶„í¬ ê·¸ë˜í”„ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. 
    g = sns.FacetGrid(df, col = 'survived')
    g.map(plt.hist, col_name, bins = 30)

    # ë‘ ì§‘ë‹¨ (survived = 1, survived = 0)ì˜ í‘œì¤€í¸ì°¨ë¥¼ ê°ê° ì¶œë ¥í•©ë‹ˆë‹¤. 
    titanic_survived = df[df['survived'] == 1]
    titanic_survived_static = np.array(titanic_survived[col_name])
    print("data std id", "%.2f" % np.std(titanic_survived_static))
    titanic_n_survived = df[df['survived'] == 0] 
    titanic_n_survived_static = np.array(titanic_n_survived[col_name])
    print("data std id", "%.2f" % np.std(titanic_n_survived_static))

    # T-testë¡œ ë‘ ì§‘ë‹¨ì˜ í‰ê·  ì°¨ì´ë¥¼ ê²€ì •í•©ë‹ˆë‹¤. 
    tTestResult = stats.ttest_ind(titanic_survived[col_name], titanic_n_survived[col_name])
    tTestResultDiffVar = stats.ttest_ind(titanic_survived[col_name], titanic_n_survived[col_name], equal_var = False)
    print("The t-statistic and p-value assuming equal variances is %.3f and %.3f." % tTestResult)
    print("The t-statistic and p-value not assuming equal variances is %.3f and %.3f." % tTestResultDiffVar)

    if distribution_check : 
        # Shapiro-Wilk ê²€ì • : ë¶„í¬ì˜ ì •ê·œì„± ì •ë„ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤. 
        print("The w-statistic and p-value in Survived %.3f and %.3f" % stats.shapiro(titanic_survived[col_name]))
        print("The w-statistic and p-value in Non-Survived %.3f and %.3f" % stats.shapiro(titanic_n_survived[col_name]))
```

ğŸ” ìë™í™” í•¨ìˆ˜ ì‹¤í–‰í•˜ê¸°


```python
# ì•ì„œ ì •ì˜í•œ valid_features í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤. age í”¼ì²˜ì™€ sibsp í”¼ì²˜ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤. 
valid_features(df_train[df_train['age'] > 0], 'age', distribution_check = True)
valid_features(df_train, 'sibsp', distribution_check = False)
```

    data std id 14.22
    data std id 13.71
    The t-statistic and p-value assuming equal variances is -0.546 and 0.585.
    The t-statistic and p-value not assuming equal variances is -0.543 and 0.587.
    The w-statistic and p-value in Survived 0.982 and 0.001
    The w-statistic and p-value in Non-Survived 0.968 and 0.000
    data std id 0.64
    data std id 1.34
    The t-statistic and p-value assuming equal variances is -2.118 and 0.034.
    The t-statistic and p-value not assuming equal variances is -2.446 and 0.015.
    


    
![png](/assets/images/Book/7/output_11_1.png)
    



    
![png](/assets/images/Book/7/output_11_2.png)
    


## ë¶„ë¥˜ : ìƒì¡´ì ë¶„ë¥˜ ëª¨ë¸ ë§Œë“¤ê¸° 

ğŸ” ë¶„ë¥˜ ëª¨ë¸ì„ ìœ„í•´ ì „ì²˜ë¦¬í•˜ê¸° 


```python
# ageì˜ ê²°ì¸¡ê°’ì„ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤. 
replace_mean = df_train[df_train['age'] > 0]['age'].mean()
df_train['age'] = df_train['age'].fillna(replace_mean)
df_test['age'] = df_test['age'].fillna(replace_mean)

# embark : 2ê°œì˜ ê²°ì¸¡ê°’ì„ ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤. 
embarked_mode = df_train['embarked'].value_counts().index[0]
df_train['embarked'] = df_train['embarked'].fillna(embarked_mode)
df_test['embarked'] = df_test['embarked'].fillna(embarked_mode)

# ì›-í•« ì¸ì½”ë”©ì„ ìœ„í•œ í†µí•© ë°ì´í„° í”„ë ˆì„(whole_df)ì„ ìƒì„±í•©ë‹ˆë‹¤. 
whole_df = pd.concat([df_train, df_test], ignore_index=True)
train_idx_num = len(df_train)

# pnadas íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•œ ì›-í•« ì¸ì½”ë”©ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. 
whole_df_encoded = pd.get_dummies(whole_df)
df_train = whole_df_encoded[:train_idx_num]
df_test = whole_df_encoded[train_idx_num:]

df_train.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pclass</th>
      <th>survived</th>
      <th>age</th>
      <th>sibsp</th>
      <th>parch</th>
      <th>fare</th>
      <th>sex_female</th>
      <th>sex_male</th>
      <th>embarked_C</th>
      <th>embarked_Q</th>
      <th>embarked_S</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2</td>
      <td>1</td>
      <td>13.000000</td>
      <td>0</td>
      <td>1</td>
      <td>19.5000</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>4.000000</td>
      <td>1</td>
      <td>1</td>
      <td>23.0000</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>1</td>
      <td>30.000000</td>
      <td>1</td>
      <td>0</td>
      <td>13.8583</td>
      <td>True</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>0</td>
      <td>30.231444</td>
      <td>0</td>
      <td>0</td>
      <td>7.7250</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>1</td>
      <td>22.000000</td>
      <td>0</td>
      <td>0</td>
      <td>7.7250</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>



ğŸ” ë¶„ë¥˜ ëª¨ë¸ë§ : ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ 


```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score 

# ë°ì´í„°ë¥¼ í•™ìŠµ ë°ì´í„°ì…‹, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤. 
x_train, y_train = df_train.loc[:, df_train.columns != 'survived'].values, df_train['survived'].values 
x_test, y_test = df_test.loc[:, df_test.columns != 'survived'].values, df_test['survived'].values 

# ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤. 
lr = LogisticRegression(random_state = 0)
lr.fit(x_train, y_train)

# í•™ìŠµí•œ ëª¨ë¸ì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. 
y_pred = lr.predict(x_test) 
y_pred_probability = lr.predict_proba(x_test)[:,1]
```

    C:\python_basic\python3.9\lib\site-packages\sklearn\linear_model\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    

ğŸ” ë¶„ë¥˜ ëª¨ë¸ í‰ê°€í•˜ê¸° 


```python
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì •í™•ë„, ì •ë°€ë„, íŠ¹ì´ë„, f1 í‰ê°€ ì§€í‘œë¥¼ ê°ê° ì¶œë ¥í•©ë‹ˆë‹¤. 
print("accuracy : %.2f" % accuracy_score(y_test, y_pred))
print("Precision : %.3f" % precision_score(y_test, y_pred))
print("Recall : %.3f" % recall_score(y_test, y_pred))
print("F1 : %.3f" % f1_score(y_test, y_pred))
```

    accuracy : 0.80
    Precision : 0.756
    Recall : 0.673
    F1 : 0.712
    


```python
from sklearn.metrics import confusion_matrix 

# Confusion Matrixë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. 
comfmat = confusion_matrix(y_true = y_test, y_pred = y_pred)
print(comfmat)
```

    [[214  32]
     [ 48  99]]
    

ğŸ” ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì˜ AUC êµ¬í•˜ê¸° 


```python
from sklearn.metrics import roc_curve, roc_auc_score 

# AUC(Area Under the Curve)ë¥¼ ê³„ì‚°í•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤. 
false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_probability) 
roc_auc = roc_auc_score(y_test, y_pred_probability)
print("AUC L %.3f" % roc_auc)

# ROC curveë¥¼ ê·¸ë˜í”„ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤. 
plt.rcParams['figure.figsize'] = [5, 4]
plt.plot(false_positive_rate, true_positive_rate, label = 'ROC curve (area = %0.3f)' % roc_auc, color = 'red', linewidth = 4.0)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC curve of Logistic regression')
plt.legend(loc = "lower right")
```

    AUC L 0.838
    




    <matplotlib.legend.Legend at 0x1ac0c32f400>




    
![png](/assets/images/Book/7/output_20_2.png)
    


ğŸ” ì˜ì‚¬ê²°ì • ë‚˜ë¬´ ëª¨ë¸ 

â–ª ì˜ì‚¬ê²°ì • ë‚˜ë¬´ ëª¨ë¸ì€ í”¼ì²˜ ë‹¨ìœ„ë¡œ ì¡°ê±´ì„ ë¶„ê¸°í•˜ì—¬ ì •ë‹µì˜ ì§‘í•©ì„ ì¢í˜€ë‚˜ê°€ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë§ˆì¹˜ ìŠ¤ë¬´ê³ ê°œë†€ì´ì—ì„œ ì •ë‹µì„ ì°¾ì•„ ë‚˜ê°€ëŠ” ê³¼ì •ê³¼ ìœ ì‚¬í•˜ë©°, ì´ë¥¼ ë„ì‹í™”í•˜ë©´ ìƒê¹€ìƒˆê°€ 'ë‚˜ë¬´ ëª¨ì–‘'ê³¼ ê°™ë‹¤ í•˜ì—¬ ë¶™ì—¬ì§„ ì´ë¦„ì…ë‹ˆë‹¤.

ğŸ” ì˜ì‚¬ê²°ì • ë‚˜ë¬´ 


```python
from sklearn.tree import DecisionTreeClassifier 

# ì˜ì‚¬ê²°ì • ë‚˜ë¬´ë¥¼ í•™ìŠµí•˜ê³ , í•™ìŠµí•œ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤. 
dtc = DecisionTreeClassifier()
dtc.fit(x_train, y_train)
y_pred = dtc.predict(x_test)
y_pred_probability = dtc.predict_proba(x_test)[:,1]

# í•™ìŠµí•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê³„ì‚°í•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤. 
print("accuracy : %.2f" % accuracy_score(y_test, y_pred))
print("Precision : %.3f" % precision_score(y_test, y_pred))
print("Recall : %.3f" % recall_score(y_test, y_pred))
print("F1 : %.3f" % f1_score(y_test, y_pred))

# í•™ìŠµí•œ ëª¨ë¸ì˜ AUCë¥¼ ê³„ì‚°í•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤. 
false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_probability)
roc_auc = roc_auc_score(y_test, y_pred_probability) 
print("AUC : %.3f" % roc_auc)

# ROC curveë¥¼ ê·¸ë˜í”„ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤. 
plt.rcParams['figure.figsize'] = [5, 4]
plt.plot(false_positive_rate, true_positive_rate, label = 'ROC curve (area = %0.3f)' % roc_auc, color = 'red', linewidth = 4.0)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC curve of Logistic regression')
plt.legend(loc = "lower right")
```

    accuracy : 0.76
    Precision : 0.688
    Recall : 0.646
    F1 : 0.667
    AUC : 0.744
    




    <matplotlib.legend.Legend at 0x1ac0c386b80>




    
![png](/assets/images/Book/7/output_22_2.png)
    


## ëª¨ë¸ ê°œì„  : í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì²«ê±¸ìŒ

ğŸ” ë¶„ë¥˜ ëª¨ë¸ì„ ìœ„í•´ ë‹¤ì‹œ ì „ì²˜ë¦¬í•˜ê¸° 


```python
# ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. 
df_train = pd.read_csv("./data/titanic_train.csv")
df_test = pd.read_csv("./data/titanic_test.csv")
df_train = df_train.drop(['ticket', 'body', 'home.dest'], axis = 1)
df_test = df_test.drop(['ticket', 'body', 'home.dest'], axis = 1)

# ageì˜ ê²°ì¸¡ê°’ì„ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤. 
replace_mean = df_train[df_train['age'] > 0]['age'].mean()
df_train['age'] = df_train['age'].fillna(replace_mean)
df_test['age'] = df_test['age'].fillna(replace_mean)

# embark : 2ê°œì˜ ê²°ì¸¡ê°’ì„ ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤. 
embarked_mode = df_train['embarked'].value_counts().index[0]
df_train['embarked'] = df_train['embarked'].fillna(embarked_mode)
df_test['embarked'] = df_test['embarked'].fillna(embarked_mode)

# ì›-í•« ì¸ì½”ë”©ì„ ìœ„í•œ í†µí•© ë°ì´í„° í”„ë ˆì„(whole_df)ì„ ìƒì„±í•©ë‹ˆë‹¤. 
whole_df = pd.concat([df_train, df_test], ignore_index=True)
train_idx_num = len(df_train)
```

ğŸ” cabin í”¼ì²˜ í™œìš©í•˜ê¸°


```python
print(whole_df['cabin'].value_counts()[:10])
```

    cabin
    C23 C25 C27        6
    B57 B59 B63 B66    5
    G6                 5
    F4                 4
    B96 B98            4
    F33                4
    C78                4
    D                  4
    F2                 4
    C22 C26            4
    Name: count, dtype: int64
    

ğŸ” cabin í”¼ì²˜ í™œìš©í•˜ê¸°


```python
# ê²°ì¸¡ ë°ì´í„°ì˜ ê²½ìš°ëŠ” 'X'ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤. 
whole_df['cabin'] = whole_df['cabin'].fillna('X')

# cabin í”¼ì²˜ì˜ ì²« ë²ˆì¬ ì•ŒíŒŒë²³ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. 
whole_df['cabin'] = whole_df['cabin'].apply(lambda x : x[0])

# ì¶”ì¶œí•œ ì•ŒíŒŒë²³ ì¤‘, Gì™€ TëŠ” ìˆ˜ê°€ ë„ˆë¬´ ì‘ê¸° ë•Œë¬¸ì— ë§ˆì°¬ê°€ì§€ë¡œ 'X'ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤. 
whole_df['cabin'] = whole_df['cabin'].replace({"G" : "X", "T" : "X"})

ax = sns.countplot(x = 'cabin', hue = 'survived', data = whole_df)
plt.show()
```


    
![png](/assets/images/Book/7/output_28_0.png)
    


ğŸ” name í”¼ì²˜ í™œìš©í•˜ê¸° 


```python
# ì´ë¦„ì—ì„œ í˜¸ì¹­ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. 
name_grade = whole_df['name'].apply(lambda x : x.split(", ", 1)[1].split(".")[0])
name_grade = name_grade.unique().tolist()
print(name_grade)
```

    ['Miss', 'Mr', 'Master', 'Mrs', 'Dr', 'Mlle', 'Col', 'Rev', 'Ms', 'Mme', 'Sir', 'the Countess', 'Dona', 'Jonkheer', 'Lady', 'Major', 'Don', 'Capt']
    

ğŸ” name í”¼ì²˜ í™œìš©í•˜ê¸° 


```python
# í˜¸ì¹­ì— ë”°ë¼ ì‚¬íšŒì  ì§€ìœ„(1901ë…„ëŒ€ ê¸°ì¤€)ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. 
grade_dict = {'A' : ['Rev', 'Col', 'Major', 'Dr', 'Capt', 'Sir'], # ëª…ì˜ˆì§ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. 
              'B' : ['Ms', 'Mme', 'Mrs', 'Dona'],                 # ì—¬ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. 
              'C' : ['Jonkheer', 'the Countess'],                 # ê·€ì¡±ì´ë‚˜ ì‘ìœ„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. 
              'D' : ['Mr', 'Don'],                                # ë‚¨ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. 
              'E' : ['Master'],                                   # ì Šì€ ë‚¨ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. 
              'F' : ['Miss', 'Mile', 'Lady']}                     # ì Šì€ ì—¬ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. 

# ì •ì˜í•œ í˜¸ì¹­ì˜ ê¸°ì¤€ì— ë”°ë¼ A ~ Fì˜ ë¬¸ìë¡œ name í”¼ì²˜ë¥¼ ë‹¤ì‹œ ì •ì˜í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. 
def give_grade(x) : 
    grade = x.split(", ", 1)[1].split(".")[0]
    for key, value in grade_dict.items() : 
        for title in value : 
            if grade == title : 
                return key 
    return 'G'

# ìœ„ì˜ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ name í”¼ì²˜ë¥¼ ìƒˆë¡­ê²Œ ì •ì˜í•©ë‹ˆë‹¤. 
whole_df['name'] = whole_df['name'].apply(lambda x : give_grade(x))
print(whole_df['name'].value_counts())
```

    name
    D    758
    F    261
    B    201
    E     61
    A     24
    G      2
    C      2
    Name: count, dtype: int64
    

ğŸ” ì›-í•« ì¸ì½”ë”© 


```python
# íŒë‹¤ìŠ¤ íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•œ ì›-í•« ì¸ì½”ë”©ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. 
whole_df_encoded = pd.get_dummies(whole_df)
df_train = whole_df_encoded[:train_idx_num]
df_test = whole_df_encoded[train_idx_num:]
df_train.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pclass</th>
      <th>survived</th>
      <th>age</th>
      <th>sibsp</th>
      <th>parch</th>
      <th>fare</th>
      <th>name_A</th>
      <th>name_B</th>
      <th>name_C</th>
      <th>name_D</th>
      <th>...</th>
      <th>cabin_A</th>
      <th>cabin_B</th>
      <th>cabin_C</th>
      <th>cabin_D</th>
      <th>cabin_E</th>
      <th>cabin_F</th>
      <th>cabin_X</th>
      <th>embarked_C</th>
      <th>embarked_Q</th>
      <th>embarked_S</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2</td>
      <td>1</td>
      <td>13.000000</td>
      <td>0</td>
      <td>1</td>
      <td>19.5000</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>4.000000</td>
      <td>1</td>
      <td>1</td>
      <td>23.0000</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>1</td>
      <td>30.000000</td>
      <td>1</td>
      <td>0</td>
      <td>13.8583</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>0</td>
      <td>30.231444</td>
      <td>0</td>
      <td>0</td>
      <td>7.7250</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>1</td>
      <td>22.000000</td>
      <td>0</td>
      <td>0</td>
      <td>7.7250</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 25 columns</p>
</div>



ğŸ” í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì´ ì™„ë£Œëœ ë°ì´í„°ì…‹ í•™ìŠµ 


```python
# ë°ì´í„°ë¥¼ í•™ìŠµ ë°ì´í„°ì…‹, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤. 
x_train, y_train = df_train.loc[:, df_train.columns != 'survived'].values, df_train['survived'].values 
x_test, y_test = df_test.loc[:, df_test.columns != 'survived'].values, df_test['survived'].values 

# ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤. 
lr = LogisticRegression(random_state = 0)
lr.fit(x_train, y_train)

# í•™ìŠµëœ ëª¨ë¸ì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. 
y_pred = lr.predict(x_test)
y_pred_probability = lr.predict_proba(x_test)[:, 1]

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ accuracy, precision, recall, f1 í‰ê°€ ì§€í‘œë¥¼ ê°ê° ì¶œë ¥í•©ë‹ˆë‹¤. 
print("accuracy : %.2f" % accuracy_score(y_test, y_pred))
print("Precision : %.3f" % precision_score(y_test, y_pred))
print("Recall : %.3f" % recall_score(y_test, y_pred))
print("F1 : %.3f" % f1_score(y_test, y_pred))  # AUC (Area Under the Curve) & ROC curve

# AUC (Area Under the Curve)ë¥¼ ê³„ì‚°í•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤. 
false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_probability) 
roc_auc = roc_auc_score(y_test, y_pred_probability) 
print("AUC L %.3f" % roc_auc)

# ROC curveë¥¼ ê·¸ë˜í”„ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤. 
plt.rcParams['figure.figsize'] = [5, 4]
plt.plot(false_positive_rate, true_positive_rate, label = 'ROC curve (area = %0.3f)' % roc_auc, color = 'red', linewidth = 4.0) 
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC curve of Logistic regression')
plt.legend(loc = "lower right")
```

    accuracy : 0.80
    Precision : 0.739
    Recall : 0.714
    F1 : 0.727
    AUC L 0.852
    

    C:\python_basic\python3.9\lib\site-packages\sklearn\linear_model\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    




    <matplotlib.legend.Legend at 0x1ac0c606f10>




    
![png](/assets/images/Book/7/output_36_3.png)
    


ğŸ” í”¼ì²˜ ì˜í–¥ë ¥ ì‚´í´ë³´ê¸° 


```python
# ì˜ˆì¸¡ ëŒ€ìƒì¸ survived í”¼ì²˜ë¥¼ ì œì™¸í•œ ëª¨ë“  í”¼ì²˜ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. (ê·¸ë˜í”„ì˜ yì¶•)
cols = df_train.columns.tolist()
cols.remove('survived')
y_pos = np.arange(len(cols))

# ê° í”¼ì²˜ë³„ íšŒê·€ ë¶„ì„ ê³„ìŠ¤ë¥¼ ê·¸ë˜í”„ì˜ xì¶•ìœ¼ë¡œ í•˜ì—¬ í”¼ì²˜ ì˜í–¥ë ¥ ê·¸ë˜í”„ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. 
plt.rcParams['figure.figsize'] = [10, 8]
fig, ax = plt.subplots()
ax.barh(y_pos, lr.coef_[0], align = 'center', color = 'green', ecolor = 'black')
ax.set_yticks(y_pos)
ax.set_yticklabels(cols)
ax.invert_yaxis()
ax.set_xlabel('Coef')
ax.set_title("Each Feature's Coef")

plt.show()
```


    
![png](/assets/images/Book/7/output_38_0.png)
    


## í‰ê°€ : ëª¨ë¸ ê²€ì¦í•˜ê¸°

ğŸ” K-fold êµì°¨ ê²€ì¦ 

â–ª í•™ìŠµìš© ë°ì´í„°ì…‹ê³¼ í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹ì„ ë‚˜ëˆŒ ë•Œ, ë‘ ë°ì´í„°ëŠ” ë¶ˆê· ë“±í•˜ê²Œ ë‚˜ëˆ ì¡Œì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤. k-fold êµì°¨ ê²€ì¦ì€ ì´ ê°€ëŠ¥ì„±ì„ ë‚®ì¶°ì£¼ëŠ” ë°©ë²•ìœ¼ë¡œ, ë°ì´í„°ë¥¼ kê°œì˜ foldë¡œ ë‚˜ëˆ„ì–´ k-1ê°œëŠ” í•™ìŠµ ë°ì´í„°, ë‚˜ë¨¸ì§€ 1ê°œëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ì€ k = 5ì¸ k-fold êµì°¨ ê²€ì¦ì„ ë‚˜íƒ€ë‚¸ ê²ƒì…ë‹ˆë‹¤. 

![png](/assets/images/Book/7/1.png)

â–ª ì´ ê·¸ë¦¼ì—ì„œëŠ” ì´ 5ê°œì˜ í•™ìŠµì„ í†µí•´ ëª¨ë¸ì˜ ë¶„í•  ê²€ì¦ì„ 5íšŒ ë°˜ë³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë§Œì•½ ì´ kë²ˆì˜ ê²€ì¦ ê³¼ì •ì—ì„œ í…ŒìŠ¤íŠ¸ ì ìˆ˜ ê°„ì˜ ì°¨ì´ê°€ í¬ì§€ ì•Šë‹¤ë©´ ëª¨ë¸ì€ ê³¼ì í•©ì´ ì¼ì–´ë‚¬ì„ ê°€ëŠ¥ì„±ì´ ë‚®ì€ ê²ƒì…ë‹ˆë‹¤. 

ğŸ” K-fold êµì°¨ ê²€ì¦ ìˆ˜í–‰í•˜ê¸° 


```python
from sklearn.model_selection import KFold 

# K-fold êµì°¨ ê²€ì¦ì˜ kë¥¼ 5ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. 
k = 5
cv = KFold(k, shuffle = True, random_state = 0)
auc_history = []

# K-foldë¥¼ 5ë²ˆì˜ ë¶„í•  í•™ìŠµìœ¼ë¡œ ë°˜ë³µí•©ë‹ˆë‹¤. 
for i, (train_data_row, test_data_row) in enumerate(cv.split(whole_df_encoded)) : 

    # 5ê°œë¡œ ë¶„í• ëœ fold ì¤‘ 4ê°œë¥¼ í•™ìŠµ ë°ì´í„°ì…‹, 1ê°œë¥¼ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ì§€ì •í•©ë‹ˆë‹¤. ë§¤ ë°˜ë³µì‹œë§ˆë‹¤ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì€ ë³€ê²½ë©ë‹ˆë‹¤. 
    df_train = whole_df_encoded.iloc[train_data_row]
    df_test = whole_df_encoded.iloc[test_data_row]

    # survived í”¼ì²˜ë¥¼ y, ë‚˜ë¨¸ì§€ í”¼ì²˜ë“¤ì„ x ë°ì´í„°ë¡œ ì§€ì •í•©ë‹ˆë‹¤. 
    splited_x_train, splited_y_train = df_train.loc[:, df_train.columns != 'survived'].values, df_train['survived'].values 
    splited_x_test, splited_y_test = df_test.loc[:, df_test.columns != 'survived'].values, df_test['survived'].values 

    # ì£¼ì–´ì§„ ë°ì´í„°ë¡œ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤. 
    lr = LogisticRegression(random_state = 0)
    lr.fit(splited_x_train, splited_y_train)
    y_pred = lr.predict(splited_x_test)
    y_pred_probability = lr.predict_proba(splited_x_test)[:, 1]

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì˜ AUCë¥¼ ê³„ì‚°í•˜ì—¬ auc_historyì— ì €ì¥í•©ë‹ˆë‹¤. 
    false_positive_rate, true_positive_rate, thresholds = roc_curve(splited_y_test, y_pred_probability) 
    roc_auc = roc_auc_score(splited_y_test, y_pred_probability)
    auc_history.append(roc_auc)

# auc_historyì— ì €ì¥ëœ ë‹¤ì„¯ ë²ˆì˜ í•™ìŠµ ê²°ê³¼(AUC)ë¥¼ ê·¸ë˜í”„ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤. 
plt.xlabel("Each k-fold")
plt.ylabel("AUC of splited test data") 
plt.plot(range(1, k + 1), auc_history) # baseline 
```

    C:\python_basic\python3.9\lib\site-packages\sklearn\linear_model\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    C:\python_basic\python3.9\lib\site-packages\sklearn\linear_model\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    C:\python_basic\python3.9\lib\site-packages\sklearn\linear_model\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    C:\python_basic\python3.9\lib\site-packages\sklearn\linear_model\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    C:\python_basic\python3.9\lib\site-packages\sklearn\linear_model\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    




    [<matplotlib.lines.Line2D at 0x1ac0c6ebeb0>]




    
![png](/assets/images/Book/7/output_40_2.png)
    


ğŸ” í•™ìŠµ ê³¡ì„  ë¶„ì„í•˜ê¸° 


```python
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
import numpy as np

# í•™ìŠµ ê³¡ì„  ë°ì´í„° ê³„ì‚°
train_sizes, train_scores, test_scores = learning_curve(
    estimator=lr,         # í•™ìŠµí•  ëª¨ë¸
    X=x_train,            # í›ˆë ¨ ë°ì´í„°
    y=y_train,            # ì •ë‹µ ë ˆì´ë¸”
    train_sizes=np.linspace(0.1, 1.0, 5),  # í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ (10%~100%)
    cv=5,                 # 5-fold cross-validation
    scoring='accuracy',   # ì •í™•ë„ë¡œ ì„±ëŠ¥ ì¸¡ì •
    n_jobs=-1             # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©
)

# í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

# ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                 train_scores_mean + train_scores_std, alpha=0.1, color="r")

plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")
plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                 test_scores_mean + test_scores_std, alpha=0.1, color="g")

plt.title("Learning Curve")
plt.xlabel("Training Examples")
plt.ylabel("Score")
plt.legend(loc="best")
plt.grid()
plt.show()
```


    
![png](/assets/images/Book/7/output_42_0.png)
    


## í‘œë¡œ ì •ë¦¬í•˜ëŠ” ë°ì´í„° ë¶„ì„ 

|ì£¼ìš” í‚¤ì›Œë“œ|í•µì‹¬ ë‚´ìš©|ì„¤ëª…|
|-|-|-|
|ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨ë¸|ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ ì´ìš©í•œ ë¶„ë¥˜ ëª¨ë¸|ëª¨ë¸ì˜ ê²°ê³¼ì€ 0 ~ 1 ì‚¬ì´ì˜ í™•ë¥ ê°’ì„ 0, 1ë¡œ ë¶„ë¥˜í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. í”¼ì²˜ ì˜í–¥ë ¥ì„ ë¶„ì„í•˜ê¸° ìš©ì´í•˜ë‹¤ëŠ” ì¥ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.|
|ê²°ì¸¡ê°’ ì²˜ë¦¬|ëª¨ë¸ í•™ìŠµì˜ ê³¼ì •ì—ì„œ ê²°ì¸¡ê°’ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•|ê²°ì¸¡ê°’ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì€ ê²°ì¸¡ê°’ì„ ì‚­ì œí•´ë²„ë¦¬ëŠ” ë°©ë²•, ê·¸ë¦¬ê³  ì„ì˜ì˜ ìˆ˜ì¹˜ë¡œ ì±„ì›Œ ë„£ëŠ” ë°©ë²•ì´ ì¡´ì¬í•©ë‹ˆë‹¤.|
|ë¶„ë¥˜ ëª¨ë¸ì˜ í‰ê°€|Confusion Matrixë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë¶„ë¥˜ ëª¨ë¸ì˜ í‰ê°€ì§€í‘œ|Confusion Matrixë¥¼ í†µí•´ ê³„ì‚°ëœ Accuracy, Precision, Recall, F1-score, AUC ë“±ì˜ ìˆ˜ì¹˜ë¡œ ë¶„ë¥˜ ë³´ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤.|
|ë¶„ë¥˜ ëª¨ë¸ì˜ ê°œì„ |í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§|í”¼ì²˜ ì—”ì œë‹ˆì–´ë§ì´ë€ ëª¨ë¸ì— ì‚¬ìš©í•  í”¼ì²˜ë¥¼ ê°€ê³µí•˜ëŠ” ë¶„ì„ ì‘ì—…ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.|
|ë¶„ë¥˜ ëª¨ë¸ì˜ ê²€ì¦|ëª¨ë¸ì˜ ê´’ê±°í•©ì„ ê²€ì¦í•˜ëŠ” ë°©ë²•|ë¶„ë¥˜ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê²€ì¦í•˜ê¸° ìœ„í•´ì„œëŠ” ëª¨ë¸ì˜ ê³¼ì í•© ì—¬ë¶€ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤. ê·¸ ë°©ë²•ìœ¼ë¡œ K-fold êµì°¨ ê²€ì¦, í•™ìŠµ ê³¡ì„ ì˜ ê´€ì°° ë“±ì˜ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.|
